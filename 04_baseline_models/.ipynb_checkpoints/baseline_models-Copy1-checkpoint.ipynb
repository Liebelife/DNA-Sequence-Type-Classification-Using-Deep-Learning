{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ffd5d7c-a6bc-44ea-9fc5-2210f3b82972",
   "metadata": {},
   "source": [
    "- linregrese\n",
    "- SVG\n",
    "- GradientBoostingClassifier\n",
    "- neuronka (MLP) - srovnani s transformerem (taky neuronka)\n",
    "- RandomForestClassifier\n",
    "- RidgeClassifier\n",
    "\n",
    "stejny random seed, test set (i pro adapter)\n",
    "\n",
    "6 modelu vs adapter\n",
    "\n",
    "## cross validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0790e35-3b42-4541-878e-b604f3922e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a2d915fa-25c9-4ea3-9060-8986deffbfd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/lieberze/DP/Thesis/baseline\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7184c86c-04da-463d-8710-bb5c070d149e",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0fe4085-2396-415e-a117-b1a4252c0b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "RootFolder = \"/home/lieberze/DP/Thesis/\"\n",
    "\n",
    "TokenizerFolder = \"tokenizery_new_data\"\n",
    "# Name = \"02_ByteLevelBPE\"\n",
    "Name = \"01_CharBPE\"\n",
    "FolderName = \"All_genomes_sample\"\n",
    "Length = \"All_512\"\n",
    "Size = \"5000\"\n",
    "All_512_BLBPE = os.path.abspath(os.path.join(RootFolder, f'{TokenizerFolder}/{Name}/{FolderName}/{Length}/{Size}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5820ed14-255a-4800-9c12-3ab4e6021d90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "file /home/lieberze/DP/Thesis/tokenizery_new_data/01_CharBPE/All_genomes_sample/All_512/5000/config.json not found\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizer(name_or_path='/home/lieberze/DP/Thesis/tokenizery_new_data/01_CharBPE/All_genomes_sample/All_512/5000', vocab_size=5000, model_max_len=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'sep_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'pad_token': AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'cls_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True)})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# def LoadTokenizer(TokenizerPath, Tokenizer):\n",
    "#     print(TokenizerPath)\n",
    "#     vocab = f\"{TokenizerPath}/vocab.json\"\n",
    "#     merges = f\"{TokenizerPath}/merges.txt\"\n",
    "#     tokenizer = Tokenizer(vocab, merges)    \n",
    "#     return tokenizer\n",
    "# tokenizer = LoadTokenizer(All_512_BLBPE, ByteLevelBPETokenizer)\n",
    "\n",
    "from transformers import RobertaTokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained(All_512_BLBPE)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816d0f1c-9d0a-4dd6-9b82-c5aaaf54b8dd",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d7414a79-f946-4946-b554-5930b5020f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "RootFolder = \"/home/lieberze/DP/Thesis/\"\n",
    "Baseline = os.path.abspath(os.path.join(RootFolder, \"baseline\"))\n",
    "DataFolder = os.path.abspath(os.path.join(Baseline, \"data\"))\n",
    "TestFolder = os.path.abspath(os.path.join(DataFolder, \"test\"))\n",
    "TrainFolder = os.path.abspath(os.path.join(DataFolder, \"train\"))\n",
    "\n",
    "TrainFile = os.path.abspath(os.path.join(TrainFolder, \"Train_250k.txt\"))\n",
    "TestFile = os.path.abspath(os.path.join(TestFolder, \"Test_holdout_150k.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c413fc2-09ac-46ea-b715-4407957f69fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kontrola vyvazenosti datasetu\n",
      "train file:\n",
      "  83352 exon\n",
      "  83096 intergenic\n",
      "  83552 intron\n",
      "test file:\n",
      "  33340 exon\n",
      "  33413 intergenic\n",
      "  33247 intron\n"
     ]
    }
   ],
   "source": [
    "# ! head $TrainFile\n",
    "print(\"kontrola vyvazenosti datasetu\")\n",
    "print(\"train file:\")\n",
    "! cat $TrainFile | cut -f1 | sort | uniq -c\n",
    "print(\"test file:\")\n",
    "! cat $TestFile | cut -f1 | sort | uniq -c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc097ab1-436c-4df9-b4a2-3ccda62171d5",
   "metadata": {},
   "source": [
    "## Encode - pouze jednou\n",
    "- dodělat TrainFile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "46eaa35f-9742-4b64-8ee8-12438bfad24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(FileToEncodePath, EncodedFilesPath, Tokenizer):\n",
    "    with open(FileToEncodePath, \"r\") as file_in,\\\n",
    "        open(EncodedFilesPath + \"/CharBPE/encoded.txt\", \"w\") as file_out:\n",
    "        for Line in file_in:\n",
    "            LineSplit = Line.strip().split()\n",
    "            SeqType, Seq = LineSplit[0], LineSplit[-1]\n",
    "            Encoded = Tokenizer.encode(Seq, max_length=128, padding=\"max_length\", truncation=True)\n",
    "            file_out.write(SeqType + \"\\t\" + str(Encoded) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19519384-f267-4673-86ea-7d56f902cd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "encode(TrainFile, TrainFolder, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef41502-91fb-4737-8a1f-adec26dd1b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "encode(TestFile, TestFolder, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed4bc08-cc88-4ba6-9ea9-18d356e6ee07",
   "metadata": {},
   "source": [
    "## Nacist a ztokenizovat, dat do dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "894dff40-b383-4c02-85ab-026dff829b23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>sequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>tccaggacagccagggctacacagagaaaccctgtctcgaaaaaaa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>GACCTTTACTTTCACTTCTCTTCATTGGATATAATCAATGAATCAG...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>CATCATTTTTTCCTGAGTGCAGGATCCAACTTAGGTGCTTGGGATT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>TTCCTAACCCAGTATGATTATTTTCTATTCTCATTAATTGTCCTTC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>ATTAACCCTCTCCGCGCCCGGCCAGTTGCCGCCCCGAGGGGTAGAA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>1</td>\n",
       "      <td>GATTTACATCAAACATTGGTCGCCGGGGTGAAGGGACCCCTGCTTC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>1</td>\n",
       "      <td>AAAGGAAGCTTTACCAATGGTTCTCCTTTGCACTTGTATTGCCCcc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>0</td>\n",
       "      <td>GTAAACTGTACCCAAAATATTGCTCCATTTAACAGTTATGATTTAG...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>0</td>\n",
       "      <td>ACATCTTTGTAGAACTGGTTCCACCTGGTGTCTTTGTATTTTCCAT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>2</td>\n",
       "      <td>acctgtgctggggagcagtggcatgggagagagtcaagggcagaga...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       type                                           sequence\n",
       "0         2  tccaggacagccagggctacacagagaaaccctgtctcgaaaaaaa...\n",
       "1         2  GACCTTTACTTTCACTTCTCTTCATTGGATATAATCAATGAATCAG...\n",
       "2         0  CATCATTTTTTCCTGAGTGCAGGATCCAACTTAGGTGCTTGGGATT...\n",
       "3         1  TTCCTAACCCAGTATGATTATTTTCTATTCTCATTAATTGTCCTTC...\n",
       "4         0  ATTAACCCTCTCCGCGCCCGGCCAGTTGCCGCCCCGAGGGGTAGAA...\n",
       "...     ...                                                ...\n",
       "99995     1  GATTTACATCAAACATTGGTCGCCGGGGTGAAGGGACCCCTGCTTC...\n",
       "99996     1  AAAGGAAGCTTTACCAATGGTTCTCCTTTGCACTTGTATTGCCCcc...\n",
       "99997     0  GTAAACTGTACCCAAAATATTGCTCCATTTAACAGTTATGATTTAG...\n",
       "99998     0  ACATCTTTGTAGAACTGGTTCCACCTGGTGTCTTTGTATTTTCCAT...\n",
       "99999     2  acctgtgctggggagcagtggcatgggagagagtcaagggcagaga...\n",
       "\n",
       "[100000 rows x 2 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(TestFile, sep=\"\\t\", names=['type','sequence'])\n",
    "df[\"type\"] = pd.Categorical(df[\"type\"])\n",
    "df['type'] = df.type.cat.codes\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7e235853-3c5a-452e-9cca-016bfab6970e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>sequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>intron</td>\n",
       "      <td>[0, 4371, 1794, 718, 914, 335, 2064, 1206, 75,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>intron</td>\n",
       "      <td>[0, 43, 572, 451, 974, 413, 972, 1526, 352, 40...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>exon</td>\n",
       "      <td>[0, 554, 1166, 337, 379, 316, 378, 750, 565, 9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>intergenic</td>\n",
       "      <td>[0, 390, 294, 558, 3619, 426, 488, 767, 856, 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>exon</td>\n",
       "      <td>[0, 282, 399, 3639, 574, 305, 3796, 334, 634, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>intergenic</td>\n",
       "      <td>[0, 43, 282, 388, 352, 3223, 1589, 412, 2570, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>intergenic</td>\n",
       "      <td>[0, 640, 1385, 2309, 568, 310, 387, 898, 531, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>exon</td>\n",
       "      <td>[0, 43, 4133, 375, 474, 531, 310, 1031, 512, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>exon</td>\n",
       "      <td>[0, 295, 1042, 608, 313, 390, 291, 576, 1042, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>intron</td>\n",
       "      <td>[0, 308, 1018, 641, 1749, 455, 679, 335, 657, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             type                                           sequence\n",
       "0          intron  [0, 4371, 1794, 718, 914, 335, 2064, 1206, 75,...\n",
       "1          intron  [0, 43, 572, 451, 974, 413, 972, 1526, 352, 40...\n",
       "2            exon  [0, 554, 1166, 337, 379, 316, 378, 750, 565, 9...\n",
       "3      intergenic  [0, 390, 294, 558, 3619, 426, 488, 767, 856, 5...\n",
       "4            exon  [0, 282, 399, 3639, 574, 305, 3796, 334, 634, ...\n",
       "...           ...                                                ...\n",
       "99995  intergenic  [0, 43, 282, 388, 352, 3223, 1589, 412, 2570, ...\n",
       "99996  intergenic  [0, 640, 1385, 2309, 568, 310, 387, 898, 531, ...\n",
       "99997        exon  [0, 43, 4133, 375, 474, 531, 310, 1031, 512, 1...\n",
       "99998        exon  [0, 295, 1042, 608, 313, 390, 291, 576, 1042, ...\n",
       "99999      intron  [0, 308, 1018, 641, 1749, 455, 679, 335, 657, ...\n",
       "\n",
       "[100000 rows x 2 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(TestFolder + \"/encoded.txt\", sep=\"\\t\", names=['type','sequence'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c07f857a-7f0a-468f-8e3b-34e865fb0c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for path in paths  \n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def LoadData(Path):\n",
    "    SeqTypes, Ids = [], []\n",
    "    with open(Path, \"r\") as file:\n",
    "        for line in file:\n",
    "            s, i = line.strip().split(\"\\t\")\n",
    "            i = np.array(i.strip(\"[]\").split(\", \")).reshape(-1,1)\n",
    "            SeqTypes.append(s), Ids.append(i)\n",
    "            \n",
    "    # adds 1 (for both algorithms it is the id of the <pad> token, post == at the end\n",
    "    X = pad_sequences(Ids, value=1, padding='post')\n",
    "    nsamples, nx, ny = X.shape\n",
    "    X = X.reshape((nsamples,nx*ny)) # from 3 to 2 dimensions\n",
    "    y = np.array(SeqTypes)            \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "12be05cc-9d45-4eb4-8a6a-d3dfa39dd008",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X,y = LoadData(TestFolder + \"/encoded.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "0dc0988e-bd98-44c1-b25a-130c57e96449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "bd2f7ab6-4918-4b0b-b4f9-6a49e969304a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_grid= {\n",
    "    MLPClassifier(random_state=RANDOM_STATE, max_iter=300): { # trenuje se dlouho\n",
    "            'hidden_layer_sizes': [(50,), (100, 100), (100,), (200,), (200, 200)],\n",
    "            # uzpusobit velikosti vstupniho vektoru\n",
    "            'activation': ['tanh', 'relu'],\n",
    "            'solver': ['sgd', 'adam'],\n",
    "            'alpha': [0.0001, 0.001, 0.005, 0.05],\n",
    "            'learning_rate': ['constant','adaptive'],\n",
    "            'early_stopping': [True, False],\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb159dc8-e09f-417f-8a94-fac908d66ea8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b2760ff8-60f4-4795-a9b4-43313b01c7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TryClassifiers(model_grid, random_state, X_train, y_train):\n",
    "    kf = KFold(n_splits=5, random_state=0, shuffle=True)\n",
    "    BestScores = []\n",
    "    for clf, grid in model_grid.items():\n",
    "        model = RandomizedSearchCV(estimator=clf, \n",
    "                                   param_distributions=grid,\n",
    "                                   n_iter=5, \n",
    "                                   cv=kf,\n",
    "                                   verbose=1, \n",
    "                                   n_jobs=-1,\n",
    "                                   random_state=random_state, \n",
    "                                   scoring = \"f1_weighted\",\n",
    "                                  )\n",
    "        model.fit(X_train, y_train)     \n",
    "        best_score = model.best_score_\n",
    "        best_params = model.best_estimator_.get_params()\n",
    "        BestScores.append([best_score, clf, best_params])\n",
    "    return BestScores\n",
    "\n",
    "def PickBest(BestScores):    \n",
    "    top_model = sorted(BestScores, key=lambda x: x[0], reverse=True)[0]\n",
    "    # load model with best params\n",
    "    validation_score, model, params = top_model\n",
    "    model.set_params(**params)\n",
    "    print(model)\n",
    "    print(\"validation f1_score (weighted):\", validation_score)\n",
    "    model = model.fit(X_train, y_train)\n",
    "    return model\n",
    "      \n",
    "def FitPredictGetMetrics(model, X_test, y_test, PathToSave):\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # metrics \n",
    "    # acc_score = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    cm = metrics.confusion_matrix(y_test, y_pred, normalize='true')\n",
    "    report = metrics.classification_report(y_test,y_pred,digits=2, output_dict=True,zero_division=0)\n",
    "    # metrics.plot_confusion_matrix(model, X_test, y_test) \n",
    "    \n",
    "    print(\"test f1_score (weighted):\", f1)\n",
    "    \n",
    "    # report table (f1, precision, recall)\n",
    "    df_report = pd.DataFrame(report).transpose()\n",
    "    df_report.to_pickle(PathToSave + \"/report_3x3.pkl\")\n",
    "    weighted_F1_average = df_report[\"f1-score\"][\"weighted avg\"]\n",
    "    print(df_report)\n",
    "    \n",
    "    # SHOW THE DIFFERENCES\n",
    "    # exon, intron, intergenic = df_report[\"f1-score\"].exon, df_report[\"f1-score\"].intron, df_report[\"f1-score\"].intergenic\n",
    "    # exon_vs_rest_ratio = exon/(intron + intergenic)\n",
    "    # exon_vs_rest_f1_score = exon, intron + intergenic, exon_vs_rest_ratio\n",
    "    \n",
    "    # confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    df_cm = pd.DataFrame(cm)\n",
    "    df_cm.columns = ['exon', 'intergenic','intron']\n",
    "    df_cm.index = ['exon', 'intergenic','intron']\n",
    "    plt.title('Confusion Matrix, normalized', size=16)\n",
    "    sns.heatmap(df_cm, annot=True, cmap='Blues')\n",
    "    plt.savefig(PathToSave + '/confusion_matrix_3x3.png', transparent=False, dpi=80, bbox_inches=\"tight\")\n",
    "    plt.show()    \n",
    "      \n",
    "    return weighted_F1_average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "db78052b-0815-4310-b8fb-2a448348943d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "feee19a7-421b-41dd-9836-5c2e370f1d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "fadf2fa1-7a45-4d5a-b75c-325e452a7ce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "[[0.30972384141230824, MLPClassifier(max_iter=300, random_state=42), {'activation': 'tanh', 'alpha': 0.0001, 'batch_size': 'auto', 'beta_1': 0.9, 'beta_2': 0.999, 'early_stopping': True, 'epsilon': 1e-08, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'learning_rate_init': 0.001, 'max_fun': 15000, 'max_iter': 300, 'momentum': 0.9, 'n_iter_no_change': 10, 'nesterovs_momentum': True, 'power_t': 0.5, 'random_state': 42, 'shuffle': True, 'solver': 'adam', 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': False, 'warm_start': False}]]\n",
      "MLPClassifier(activation='tanh', early_stopping=True, max_iter=300,\n",
      "              random_state=42)\n",
      "validation f1_score (weighted): 0.30972384141230824\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'f1_score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-80-80a6171770d4>\u001b[0m in \u001b[0;36mFitPredictGetMetrics\u001b[0;34m(model, X_test, y_test, PathToSave)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;31m# metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;31m# acc_score = accuracy_score(y_test, y_pred)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'weighted'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0mcm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'true'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mreport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdigits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mzero_division\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'f1_score' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# measures = []\n",
    "# X, y = LoadData(Path + \"/encoded_3x3.txt\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state = RANDOM_STATE)\n",
    "\n",
    "#smaller_sample\n",
    "# X_tr, y_tr =  X_train[:train_size], y_train[:train_size]\n",
    "# X_te, y_te = X_test[:test_size], y_test[:test_size]\n",
    "\n",
    "BestScores = TryClassifiers(model_grid, RANDOM_STATE, X_train, y_train)\n",
    "print(BestScores)\n",
    "model = PickBest(BestScores)\n",
    "\n",
    "# save model\n",
    "Path = \"./Trials\"\n",
    "filename = '/classification_model_3x3_MLP.sav'\n",
    "joblib.dump(model, Path + filename)\n",
    "\n",
    "weighted_f1_average_ = FitPredictGetMetrics(model, X_test, y_test, Path)\n",
    "measures.append(weighted_f1_average_)\n",
    "\n",
    "ROCAUCcurve(model, X_train, y_train, X_test, y_test, Path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "2e7759ab-1e61-4f6b-8b05-48ffe18c8300",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.31916604761375,\n",
       "  MLPClassifier(activation='tanh', early_stopping=True, learning_rate='adaptive',\n",
       "                max_iter=300, random_state=42, solver='sgd'),\n",
       "  {'activation': 'tanh',\n",
       "   'alpha': 0.0001,\n",
       "   'batch_size': 'auto',\n",
       "   'beta_1': 0.9,\n",
       "   'beta_2': 0.999,\n",
       "   'early_stopping': True,\n",
       "   'epsilon': 1e-08,\n",
       "   'hidden_layer_sizes': (100,),\n",
       "   'learning_rate': 'adaptive',\n",
       "   'learning_rate_init': 0.001,\n",
       "   'max_fun': 15000,\n",
       "   'max_iter': 300,\n",
       "   'momentum': 0.9,\n",
       "   'n_iter_no_change': 10,\n",
       "   'nesterovs_momentum': True,\n",
       "   'power_t': 0.5,\n",
       "   'random_state': 42,\n",
       "   'shuffle': True,\n",
       "   'solver': 'sgd',\n",
       "   'tol': 0.0001,\n",
       "   'validation_fraction': 0.1,\n",
       "   'verbose': False,\n",
       "   'warm_start': False}]]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BestScores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b48166f-b074-4acc-953e-91c07f8da03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLPClassifier(activation='tanh', early_stopping=True, learning_rate='adaptive',\n",
    "              # max_iter=300, random_state=42, solver='sgd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea81d43-52ad-416f-aab1-d727dc76c26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c244370-61c0-4fd7-b744-111f737b735c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919f1704-7a5d-4f78-9cfc-94a0b251c067",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "12a2545a-e7bb-4164-8471-d25008b5a62b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-78-0155be4c8690>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneural_network\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMLPClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mxgboost\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mXGBClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSGDClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# from sklearn.naive_bayes import MultinomialNB\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "# from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "model_grid = {    \n",
    "    # https://medium.com/all-things-ai/in-depth-parameter-tuning-for-random-forest-d67bb7e920d\n",
    "    RandomForestClassifier(n_jobs=-1, random_state=RANDOM_STATE): {\n",
    "            \"n_estimators\": [150, 200, 400],\n",
    "            \"max_depth\": [1, 3, 5, 7, 9],\n",
    "            \"criterion\": [\"gini\", \"entropy\"],\n",
    "            \"min_samples_split\": [0.6, 0.8, 2, 4],\n",
    "            \"min_samples_leaf\": [0.4, 1, 2, 4, 10, 15]\n",
    "        },    \n",
    "    # https://www.analyticsvidhya.com/blog/2016/02/complete-guide-parameter-tuning-gradient-boosting-gbm-python/\n",
    "    GradientBoostingClassifier(max_features = 'sqrt', subsample = 0.8, random_state=RANDOM_STATE): {\n",
    "            \"min_samples_split\": [3000, 4000, 5000, 6000],\n",
    "            \"min_samples_leaf\": [50, 100, 150, 200],\n",
    "            \"max_depth\": [5, 6, 7, 8]\n",
    "        },        \n",
    "    RidgeClassifier(random_state=RANDOM_STATE): {\n",
    "            \"alpha\": [1e-3, 1e-2, 1e-1, 1]\n",
    "            },    \n",
    "    # https://datascience.stackexchange.com/questions/36049/how-to-adjust-the-hyperparameters-of-mlp-classifier-to-get-more-perfect-performa\n",
    "    MLPClassifier(random_state=RANDOM_STATE, max_iter=300): { # trenuje se dlouho\n",
    "            'hidden_layer_sizes':  [(100, 100), (100,), (200,), (200, 200)],  # uzpusobit velikosti vstupniho vektoru\n",
    "            'activation': ['tanh', 'relu'],\n",
    "            'solver': ['sgd', 'adam'],\n",
    "            'alpha': [0.0001, 0.001, 0.005, 0.05],\n",
    "            'learning_rate': ['constant','adaptive'],\n",
    "            'early_stopping': [True, False],\n",
    "    },\n",
    "    LogisticRegression(random_state=RANDOM_STATE): {\n",
    "        'penalty' : ['none', 'l1', 'l2'],\n",
    "        'solvers': ['newton-cg', 'lbfgs', 'liblinear'],\n",
    "        'C' : [100, 10, 1.0, 0.1, 0.01],\n",
    "    },\n",
    "    \n",
    "    SVC(): {\n",
    "    \n",
    "    },\n",
    "    \n",
    "    XGBClassifier():{\n",
    "    },\n",
    "    \n",
    "     SGDClassifier(random_state=RANDOM_STATE): { \n",
    "    },\n",
    "\n",
    "}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "45fd1451-d3cb-4d26-8dbd-04f9403995bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in model_grid.items():\n",
    "    model = k\n",
    "    grid = v\n",
    "    # print(type(model), type(grid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9aa5fb-facb-4191-9b0b-3e52ee20f136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # train file\n",
    "# df = pd.read_csv(TrainFile, sep=\"\\t\", names=['type','sequence'])\n",
    "# # nahrazeni slov cisly\n",
    "# df[\"type\"] = pd.Categorical(df[\"type\"])\n",
    "# df['type'] = df.type.cat.codes\n",
    "# df.head()\n",
    "\n",
    "# tokenized = []\n",
    "# for i in list(df.sequence):\n",
    "#     x = tokenizer.encode(i, max_length=128, padding=\"max_length\", truncation=True)\n",
    "#     tokenized.append(x)\n",
    "    \n",
    "# import pickle\n",
    "# with open('./tokenized_train.pickle', 'wb') as handle:\n",
    "#     pickle.dump(tokenized, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79244ae-bd10-42de-afc9-57893c626612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from torch.utils.data import Dataset\n",
    "# from torch.utils.data import DataLoader\n",
    "# from keras.preprocessing.sequence import pad_sequences\n",
    "# from torch.nn.utils.rnn import pad_sequence\n",
    "# from datasets import load_dataset\n",
    "\n",
    "# # defining the Dataset class\n",
    "# # https://www.geeksforgeeks.org/self-in-python-class/\n",
    "# # https://pytorch.org/tutorials/beginner/data_loading_tutorial.html\n",
    "# # https://medium.com/swlh/how-to-use-pytorch-dataloaders-to-work-with-enormously-large-text-files-bbd672e955a0\n",
    "# class data_set(Dataset):\n",
    "#     def __init__(self, data, labels, tokenizer):\n",
    "#         self.data = data\n",
    "#         self.labels = labels\n",
    "#         self.tokenizer = tokenizer\n",
    "        \n",
    "#     def __len__(self):\n",
    "#         return len(self.data)\n",
    "    \n",
    "#     def __getitem__(self, index):\n",
    "#         seq = self.data[index]\n",
    "#         lab = self.labels[index]\n",
    "#         tokenized = tokenizer.encode(seq, max_length=128, padding=\"max_length\", truncation=True)\n",
    "#         tnsr = torch.tensor(tokenized)\n",
    "        \n",
    "#         return tnsr # odstranit labely\n",
    "        \n",
    "# df = pd.read_csv(data_path, sep=\"\\t\", names=['type','sequence'])\n",
    "# dataset = data_set(df[\"sequence\"],df[\"type\"], tokenizer)\n",
    "# dataloader = DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=data_collator) #, collate_fn=lambda x: x )\n",
    "\n",
    "# data = next(iter(dataloader))\n",
    "# # data\n",
    "\n",
    "\n",
    "# # display(data)\n",
    "# # data.get(\"input_ids\")\n",
    "# # for (idx, batch) in enumerate(dataloader):\n",
    "# #     ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d01fe5-f60d-4b57-97aa-86e7dfe1cbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# df_train, df_eval = train_test_split(df, test_size=0.25, random_state=42, stratify=df[\"type\"])\n",
    "# df_train, df_holdout = train_test_split(df_train, test_size=0.1, random_state=42, stratify=df_train[\"type\"])\n",
    "\n",
    "# df_train = df_train.reset_index()\n",
    "# df_eval = df_eval.reset_index()\n",
    "# df_holdout = df_holdout.reset_index()\n",
    "\n",
    "# data_set_train = data_set(df_train[\"sequence\"],df_train[\"type\"], tokenizer)\n",
    "# data_set_eval = data_set(df_eval[\"sequence\"],df_eval[\"type\"], tokenizer)\n",
    "# # nesahat :)\n",
    "# holdout_test = data_set(df_holdout[\"sequence\"],df_holdout[\"type\"], tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95f2517-1ec1-425d-84c3-8a184ac7bc9c",
   "metadata": {},
   "source": [
    "## 2x2 a 3x3 classification"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
