{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "941a3f25-1fe8-4f68-b912-97f5130d4cc3",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Encode sample and generate pkl files (token dictionaries)\n",
    "\n",
    "Encode a sample file by all tokenizers with all vocabulary sizes. Creates a dictionary where keys are the tokens and values are the number of occurances in the encoded sequences a given file. Saved in a .pkl file in the correspodnimg folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbbdb68b-0fb9-41b2-8026-483d5ea5b750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/lieberze/DP/Thesis/tokenizery_2_attempt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "print(os.getcwd())\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9b00834-2554-4dfb-9f12-8d64eca98c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "# print(IPython.sys_info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b0eb9b-6cbd-4e30-8647-735c194aa128",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38143d9c-e55f-47cf-9c50-68616ea121b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "RootFolder = \"\"\n",
    "DataFolder = os.path.abspath(os.path.join(RootFolder, 'data/sample'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd56e95-ceef-40a6-a9d2-87cc104f0baa",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fa45900-8943-4fb8-88f2-0ea301c7a1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import CharBPETokenizer\n",
    "from tokenizers import ByteLevelBPETokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85443d0-4570-4b3d-a8e1-e3dfdd2cb18d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b00604e-9340-4da9-8352-13ff1f1c74af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LoadTokenizer(TokenizerPath, Tokenizer):\n",
    "    vocab = f\"{TokenizerPath}/vocab.json\"\n",
    "    merges = f\"{TokenizerPath}/merges.txt\"\n",
    "    tokenizer = Tokenizer(vocab, merges)    \n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "473ce9c1-ab0c-4fd4-8214-a3381838b5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TestTokenizer(Paths, VocabSizes, TestSequence, Tokenizer):\n",
    "    for Path in Paths:\n",
    "        for Size in VocabSizes:\n",
    "            TokenizerPath = f\"{Path}/{Size}/\"\n",
    "            tokenizer = LoadTokenizer(TokenizerPath, Tokenizer)\n",
    "            encoded = tokenizer.encode(TestSequence)             \n",
    "            print(TokenizerPath)\n",
    "            print(encoded.ids)\n",
    "            print(encoded.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0713e5bf-0c73-407b-8122-aa640c551d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def CountTokensAndUpdateDictionary(Dictionary, Encoded):\n",
    "    Tokens = Encoded.tokens\n",
    "    SmallTokenCountDict = Counter(Tokens)    \n",
    "    for Token, Value in SmallTokenCountDict.items():\n",
    "        if Token in Dictionary.keys():\n",
    "            ValueInTheBigDictionary = int(Dictionary.get(Token)) + Value\n",
    "            Dictionary[Token] = ValueInTheBigDictionary\n",
    "        else:\n",
    "            Dictionary[Token] = Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc6aae5e-58df-4c0e-85d5-bd29ec0ab76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SortAndPickle(Dictionary, Name, FolderPath):\n",
    "    Dictionary = {k: v for k, v in sorted(Dictionary.items(), key=lambda item: item[1], reverse=True)}\n",
    "    pickle.dump(Dictionary, open(f\"{FolderPath}/{Name}.pkl\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "743b3a41-1a13-4ad7-8853-85ba941f69bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def EncodeAndCountTokenOccurences(FolderPath, FilePath, Tokenizer):\n",
    "    Tokenizer = LoadTokenizer(FolderPath, Tokenizer)\n",
    "    exons, introns, intergenic = {}, {}, {}\n",
    "    with open(FilePath, \"r\") as file_in:\n",
    "        for Line in file_in:\n",
    "            LineSplit = Line.strip().split()\n",
    "            SeqType, Seq = LineSplit[0], LineSplit[-1]\n",
    "            Encoded = Tokenizer.encode(Seq)          \n",
    "            if SeqType == \"exon\":\n",
    "                CountTokensAndUpdateDictionary(exons, Encoded)\n",
    "            elif SeqType == \"intron\":\n",
    "                CountTokensAndUpdateDictionary(introns, Encoded)\n",
    "            elif SeqType == \"intergenic\":\n",
    "                CountTokensAndUpdateDictionary(intergenic, Encoded)\n",
    "        for Name, Dict in {\"exons\":exons, \"introns\":introns, \"intergenic\":intergenic}.items():       \n",
    "            SortAndPickle(Dict, Name, FolderPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6804fa28-63eb-43dc-a2d0-0038228679b0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Apply"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d13367-f56a-452a-8000-c6a63be9c8e5",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "461cc449-2042-4aad-9807-4184ba2eae8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "RootFolder = \"\"\n",
    "DataFolder = os.path.abspath(os.path.join(RootFolder, 'data/sample/Encoding'))\n",
    "\n",
    "FileToEncode = os.path.abspath(os.path.join(DataFolder, 'All_equal_shuffled_100k.txt'))\n",
    "\n",
    "FolderName = \"All_genomes_sample\"\n",
    "name = \"01_CharBPE\"\n",
    "CharBPE = os.path.abspath(os.path.join(RootFolder, f'{name}/'))\n",
    "All_1000_BPE = os.path.abspath(os.path.join(CharBPE, f'{FolderName}/All_1000/'))\n",
    "All_512_BPE = os.path.abspath(os.path.join(CharBPE, f'{FolderName}/All_512/'))\n",
    "\n",
    "name = \"02_ByteLevelBPE\"\n",
    "ByteLevelBPE = os.path.abspath(os.path.join(RootFolder, f'{name}/'))\n",
    "All_1000_BLBPE = os.path.abspath(os.path.join(ByteLevelBPE, f'{FolderName}/All_1000/'))\n",
    "All_512_BLBPE = os.path.abspath(os.path.join(ByteLevelBPE, f'{FolderName}/All_512/'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d28920-5236-42cf-b7fe-118112b18fe9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### All genomes 41.6 MB sample (all equal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585de43e-7c6f-4c34-bcc0-6bf228b65d33",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Encoding test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b897fada-1f4d-4409-b679-b2c76ca94470",
   "metadata": {},
   "source": [
    "BPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab5fc25a-aedd-4f3d-89fe-8a8c391f8741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths = [All_1000_BPE, All_512_BPE]\n",
    "# VocabSizes = [5000, 15000, 50000]\n",
    "# Tokenizer = CharBPETokenizer\n",
    "# TestSequence = \"GCGTGATTACGAGTCGTGGCAAATTTGGTCTGGCTGTGGTCTAGACATTCCAGGCGGTGCGTCTGCTCTCGGGTGCCTCTA\"\n",
    "\n",
    "# TestTokenizer(Paths, VocabSizes, TestSequence, Tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30211a28-551e-4c8d-ab9c-8e8535166abb",
   "metadata": {},
   "source": [
    "BLBPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "68c0f8f4-bcd5-44d7-bf77-488f60c7f237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths = [All_1000_BLBPE, All_512_BLBPE]\n",
    "# VocabSizes = [5000, 15000, 50000]\n",
    "# Tokenizer = ByteLevelBPETokenizer\n",
    "# TestSequence = \"GCGTGATTACGAGTCGTGGCAAATTTGGTCTGGCTGTGGTCTAGACATTCCAGGCGGTGCGTCTGCTCTCGGGTGCCTCTA\"\n",
    "\n",
    "# TestTokenizer(Paths, VocabSizes, TestSequence, Tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6efedc48-ce71-42f7-9ffa-3b06b355d65d",
   "metadata": {},
   "source": [
    "#### Encode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cb04ff-5014-48ab-b226-0e9481160087",
   "metadata": {},
   "source": [
    "##### CharBPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b559b85f-c90c-4a66-8652-48944a729e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Paths = [All_1000_BPE, All_512_BPE]\n",
    "VocabSizes = [5000, 15000, 50000]\n",
    "Tokenizer = CharBPETokenizer\n",
    "TestFile = FileToEncode\n",
    "\n",
    "for Path in Paths:\n",
    "    for Size in VocabSizes:\n",
    "        TokenizerPath = f\"{Path}/{Size}/\"\n",
    "        EncodeAndCountTokenOccurences(TokenizerPath, TestFile, Tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a578252b-3e8d-492b-ac10-cbe565413137",
   "metadata": {},
   "source": [
    "##### ByteLevelBPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "84572a45-378f-4c6d-bbe9-5fcd3202c9d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/lieberze/DP/Thesis/tokenizery_2_attempt/02_ByteLevelBPE/All_genomes_sample/All_1000/5000/\n",
      "/home/lieberze/DP/Thesis/tokenizery_2_attempt/02_ByteLevelBPE/All_genomes_sample/All_1000/15000/\n",
      "/home/lieberze/DP/Thesis/tokenizery_2_attempt/02_ByteLevelBPE/All_genomes_sample/All_1000/50000/\n",
      "/home/lieberze/DP/Thesis/tokenizery_2_attempt/02_ByteLevelBPE/All_genomes_sample/All_512/5000/\n",
      "/home/lieberze/DP/Thesis/tokenizery_2_attempt/02_ByteLevelBPE/All_genomes_sample/All_512/15000/\n",
      "/home/lieberze/DP/Thesis/tokenizery_2_attempt/02_ByteLevelBPE/All_genomes_sample/All_512/50000/\n"
     ]
    }
   ],
   "source": [
    "Paths = [All_1000_BLBPE, All_512_BLBPE]\n",
    "VocabSizes = [5000, 15000, 50000]\n",
    "Tokenizer = ByteLevelBPETokenizer\n",
    "TestFile = FileToEncode\n",
    "\n",
    "for Path in Paths:\n",
    "    for Size in VocabSizes:\n",
    "        TokenizerPath = f\"{Path}/{Size}/\"\n",
    "        print(TokenizerPath)\n",
    "        EncodeAndCountTokenOccurences(TokenizerPath, TestFile, Tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64746c4f-31e4-4e8f-9d70-139371b9960b",
   "metadata": {},
   "source": [
    "### \\<unk> token detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ae00def0-52d1-4370-820e-1a1ac162fa9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CheckPattern(path, pattern):\n",
    "    Dict = pickle.load(open(path, 'rb'))\n",
    "    Tokens = Dict.keys()\n",
    "    if pattern in Tokens:\n",
    "        # print(f\"pattern {pattern} is present in file {path}\")\n",
    "        return 1\n",
    "    else:\n",
    "        # print(f\"pattern {pattern} is NOT present in file {path}\")\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2c529210-0dd3-4fce-afc9-0a440d089791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there is NO file which has the <unk> token\n"
     ]
    }
   ],
   "source": [
    "Paths = [All_1000_BPE, All_512_BPE, All_1000_BLBPE, All_512_BLBPE]\n",
    "VocabSizes = [5000, 15000, 50000]\n",
    "SeqTypes = [\"exons\", \"introns\", \"intergenic\"]\n",
    "\n",
    "token_to_check_the_presence_of = \"<unk>\"\n",
    "\n",
    "UNK = []\n",
    "for Path in Paths:\n",
    "    for Size in VocabSizes:\n",
    "        for Seq in SeqTypes:\n",
    "            Pth = f\"{Path}/{Size}/{Seq}.pkl\"\n",
    "            PatternPresent = CheckPattern(Pth, token_to_check_the_presence_of)\n",
    "            UNK.append(PatternPresent)\n",
    "            \n",
    "if 1 in UNK:\n",
    "    print(f\"there is some file which has the {token_to_check_the_presence_of} token\")\n",
    "else:\n",
    "    print(f\"there is NO file which has the {token_to_check_the_presence_of} token\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
