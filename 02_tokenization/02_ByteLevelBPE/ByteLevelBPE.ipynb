{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf4fa4cd-1b60-4cd9-a777-39751e8b5b5e",
   "metadata": {},
   "source": [
    "# ByteLevelBPE tokenizer\n",
    "\n",
    "**This file differs from other tokenizers only in the chunk in the *Tokenizer* section here. It is therefore made to be easy to modify for other tokenization algorithms.**\n",
    "\n",
    "Useful links:\n",
    "\n",
    "- https://pypi.org/project/tokenizers/\n",
    "\n",
    "- https://www.freecodecamp.org/news/train-algorithms-from-scratch-with-hugging-face/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99401ceb-6cc5-4185-a207-e1fc3a9cbbc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/lieberze/DP/Thesis/tokenizery_new_data/02_ByteLevelBPE\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95341fae-ed31-4dd7-881f-fc6f57891630",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db1b1298-b7bd-4f90-87de-e576da64c18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import ByteLevelBPETokenizer\n",
    "Tokenizer = ByteLevelBPETokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7300e6fa-32e9-4b19-909a-bf6155f000d9",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "796ecd36-59f7-4fb5-8a73-9d476e680e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "def get_tokenizer(FilePath, SaveTo, TokenizerType, VocabSize):\n",
    "\n",
    "    path = os.path.abspath(os.path.join(SaveTo, f'{VocabSize}'))\n",
    "    \n",
    "    # create a folder if it doesn't already exist\n",
    "    !mkdir -p {path}\n",
    "    \n",
    "    tokenizer = TokenizerType\n",
    "    tokenizer.pre_tokenizer = Whitespace()\n",
    "    tokenizer.train(FilePath, vocab_size=VocabSize, min_frequency=2,\\\n",
    "                        special_tokens=[\n",
    "                                        \"<s>\",\n",
    "                                        \"<pad>\",\n",
    "                                        \"</s>\",\n",
    "                                        \"<unk>\",\n",
    "                                        \"<mask>\",\n",
    "                                    ])    \n",
    "    tokenizer.save_model(path)\n",
    "    print(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574647fc-7a94-4c96-bef7-fa32f66e0f1d",
   "metadata": {},
   "source": [
    "### Paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24e19fa-c431-490c-87df-12b30c446504",
   "metadata": {
    "tags": []
   },
   "source": [
    "## All genomes 41.6 MB sample (all equal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e246f318-04d5-4768-9cee-38c4bf94146f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d217af2b-4d1d-4ff8-b950-bdcb51f67c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "RootFolder = \"\"\n",
    "DataFolder = os.path.abspath(os.path.join(RootFolder, '../data/sample'))\n",
    "\n",
    "All_genomes_sample = os.path.abspath(os.path.join(DataFolder, 'All_genomes_sample/'))\n",
    "All_genomes_tokenizer_folder = os.path.abspath(os.path.join(RootFolder, 'All_genomes_sample/'))\n",
    "\n",
    "All_genomes_sample_512 = os.path.abspath(os.path.join(All_genomes_sample, 'All_200k_tail_for_tokenizer_512bp.raw'))\n",
    "exons_raw_512 = os.path.abspath(os.path.join(All_genomes_sample, 'exons_512.raw'))\n",
    "introns_raw_512 = os.path.abspath(os.path.join(All_genomes_sample, 'introns_512.raw'))\n",
    "intergenic_raw_512 = os.path.abspath(os.path.join(All_genomes_sample, 'intergenic_512.raw'))\n",
    "\n",
    "name = \"All\"\n",
    "All_512 = os.path.abspath(os.path.join(All_genomes_tokenizer_folder, f'{name}_512/'))\n",
    "# All_1000 = os.path.abspath(os.path.join(All_genomes_tokenizer_folder, f'{name}_1000/'))\n",
    "name = \"Introns\"\n",
    "Introns_512 = os.path.abspath(os.path.join(All_genomes_tokenizer_folder, f'{name}_512/'))\n",
    "# Introns_1000 = os.path.abspath(os.path.join(All_genomes_tokenizer_folder, f'{name}_1000/'))\n",
    "name = \"Exons\"\n",
    "Exons_512 = os.path.abspath(os.path.join(All_genomes_tokenizer_folder, f'{name}_512/'))\n",
    "# Exons_1000 = os.path.abspath(os.path.join(All_genomes_tokenizer_folder, f'{name}_1000/'))\n",
    "name = \"Intergenic\"\n",
    "Intergenic_512 = os.path.abspath(os.path.join(All_genomes_tokenizer_folder, f'{name}_512/'))\n",
    "# Intergenic_1000 = os.path.abspath(os.path.join(All_genomes_tokenizer_folder, f'{name}_1000/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28f72ee4-bf11-4058-a29c-3b0f56c80301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(All_genomes_sample_512, \"r\") as file:\n",
    "#     print(file.readline())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97a77d5-c188-4d57-b308-3b00884d197e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### All_512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a29c8784-29c3-42f2-9d81-2bbffcdaaf7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer(vocabulary_size=5000, model=ByteLevelBPE, add_prefix_space=False, lowercase=False, dropout=None, unicode_normalizer=None, continuing_subword_prefix=None, end_of_word_suffix=None, trim_offsets=False)\n",
      "Tokenizer(vocabulary_size=15000, model=ByteLevelBPE, add_prefix_space=False, lowercase=False, dropout=None, unicode_normalizer=None, continuing_subword_prefix=None, end_of_word_suffix=None, trim_offsets=False)\n",
      "Tokenizer(vocabulary_size=50000, model=ByteLevelBPE, add_prefix_space=False, lowercase=False, dropout=None, unicode_normalizer=None, continuing_subword_prefix=None, end_of_word_suffix=None, trim_offsets=False)\n"
     ]
    }
   ],
   "source": [
    "Filename = All_genomes_sample_512\n",
    "Folder = All_512\n",
    "Tokenizer = Tokenizer\n",
    "\n",
    "for Size in [5000, 15000, 50000]:\n",
    "    VocabSize = Size\n",
    "    get_tokenizer(Filename, Folder,  Tokenizer, VocabSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7a0516d-8e2a-4d54-98da-9cf1f1a35d9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer(vocabulary_size=5000, model=ByteLevelBPE, add_prefix_space=False, lowercase=False, dropout=None, unicode_normalizer=None, continuing_subword_prefix=None, end_of_word_suffix=None, trim_offsets=False)\n",
      "Tokenizer(vocabulary_size=15000, model=ByteLevelBPE, add_prefix_space=False, lowercase=False, dropout=None, unicode_normalizer=None, continuing_subword_prefix=None, end_of_word_suffix=None, trim_offsets=False)\n",
      "Tokenizer(vocabulary_size=50000, model=ByteLevelBPE, add_prefix_space=False, lowercase=False, dropout=None, unicode_normalizer=None, continuing_subword_prefix=None, end_of_word_suffix=None, trim_offsets=False)\n",
      "Tokenizer(vocabulary_size=5000, model=ByteLevelBPE, add_prefix_space=False, lowercase=False, dropout=None, unicode_normalizer=None, continuing_subword_prefix=None, end_of_word_suffix=None, trim_offsets=False)\n",
      "Tokenizer(vocabulary_size=15000, model=ByteLevelBPE, add_prefix_space=False, lowercase=False, dropout=None, unicode_normalizer=None, continuing_subword_prefix=None, end_of_word_suffix=None, trim_offsets=False)\n",
      "Tokenizer(vocabulary_size=50000, model=ByteLevelBPE, add_prefix_space=False, lowercase=False, dropout=None, unicode_normalizer=None, continuing_subword_prefix=None, end_of_word_suffix=None, trim_offsets=False)\n",
      "Tokenizer(vocabulary_size=5000, model=ByteLevelBPE, add_prefix_space=False, lowercase=False, dropout=None, unicode_normalizer=None, continuing_subword_prefix=None, end_of_word_suffix=None, trim_offsets=False)\n",
      "Tokenizer(vocabulary_size=15000, model=ByteLevelBPE, add_prefix_space=False, lowercase=False, dropout=None, unicode_normalizer=None, continuing_subword_prefix=None, end_of_word_suffix=None, trim_offsets=False)\n",
      "Tokenizer(vocabulary_size=50000, model=ByteLevelBPE, add_prefix_space=False, lowercase=False, dropout=None, unicode_normalizer=None, continuing_subword_prefix=None, end_of_word_suffix=None, trim_offsets=False)\n"
     ]
    }
   ],
   "source": [
    "Filenames = [exons_raw_512, introns_raw_512, intergenic_raw_512]\n",
    "Folders = [Exons_512, Introns_512, Intergenic_512]\n",
    "Tokenizer = Tokenizer\n",
    "\n",
    "for Filename, Folder in zip(Filenames, Folders):\n",
    "    for Size in [5000, 15000, 50000]:\n",
    "        VocabSize = Size\n",
    "        get_tokenizer(Filename, Folder,  Tokenizer, VocabSize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6126c0-57de-40e1-971a-ddd0faaccf88",
   "metadata": {
    "tags": []
   },
   "source": [
    "### All_1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67c9a93d-3cbd-49bf-88de-a3e0382be935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filename = All_genomes_sample_1000\n",
    "# Folder = All_1000\n",
    "# Tokenizer = Tokenizer\n",
    "\n",
    "# for Size in [5000, 15000, 50000]:\n",
    "#     VocabSize = Size\n",
    "#     get_tokenizer(Filename, Folder,  Tokenizer, VocabSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c00a34a-9723-461e-8a8b-345d057aa4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filenames = [exons_raw_1000, introns_raw_1000, intergenic_raw_1000]\n",
    "# Folders = [Exons_1000, Introns_1000, Intergenic_1000]\n",
    "# Tokenizer = Tokenizer\n",
    "\n",
    "# for Filename, Folder in zip(Filenames, Folders):\n",
    "#     for Size in [5000, 15000, 50000]:\n",
    "#         VocabSize = Size\n",
    "#         get_tokenizer(Filename, Folder,  Tokenizer, VocabSize)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
