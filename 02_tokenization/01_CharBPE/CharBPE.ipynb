{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf4fa4cd-1b60-4cd9-a777-39751e8b5b5e",
   "metadata": {},
   "source": [
    "# CharBPE tokenizer\n",
    "\n",
    "**This file differs from other tokenizers only in the chunk in the *Tokenizer* section here. It is therefore made to be easy to modify for other tokenization algorithms.**\n",
    "\n",
    "Useful links:\n",
    "\n",
    "- https://pypi.org/project/tokenizers/\n",
    "\n",
    "- https://www.freecodecamp.org/news/train-algorithms-from-scratch-with-hugging-face/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99401ceb-6cc5-4185-a207-e1fc3a9cbbc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/lieberze/DP/Thesis/tokenizery_new_data/01_CharBPE\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95341fae-ed31-4dd7-881f-fc6f57891630",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db1b1298-b7bd-4f90-87de-e576da64c18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import CharBPETokenizer\n",
    "Tokenizer = CharBPETokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7300e6fa-32e9-4b19-909a-bf6155f000d9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "796ecd36-59f7-4fb5-8a73-9d476e680e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "def get_tokenizer(FilePath, SaveTo, TokenizerType, VocabSize):\n",
    "\n",
    "    path = os.path.abspath(os.path.join(SaveTo, f'{VocabSize}'))\n",
    "    \n",
    "    # create a folder if it doesn't already exist\n",
    "    !mkdir -p {path}\n",
    "    \n",
    "    tokenizer = TokenizerType\n",
    "    tokenizer.pre_tokenizer = Whitespace()\n",
    "    tokenizer.train(FilePath, vocab_size=VocabSize, min_frequency=2,\\\n",
    "                        special_tokens=[\n",
    "                                        \"<s>\",\n",
    "                                        \"<pad>\",\n",
    "                                        \"</s>\",\n",
    "                                        \"<unk>\",\n",
    "                                        \"<mask>\",\n",
    "                                    ])    \n",
    "    tokenizer.save_model(path)\n",
    "    print(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006a1ae2-4aa4-4e3f-9b6e-9b3304724b38",
   "metadata": {
    "tags": []
   },
   "source": [
    "## All genomes 41.6 MB sample (all equal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646a809a-3af9-4e72-a9ba-4be2ba6acbf2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb85d428-a905-4097-bb03-aea2adc075c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "RootFolder = \"\"\n",
    "DataFolder = os.path.abspath(os.path.join(RootFolder, '../data/sample'))\n",
    "\n",
    "All_genomes_sample = os.path.abspath(os.path.join(DataFolder, 'All_genomes_sample/'))\n",
    "All_genomes_tokenizer_folder = os.path.abspath(os.path.join(RootFolder, 'All_genomes_sample/'))\n",
    "\n",
    "All_genomes_sample_512 = os.path.abspath(os.path.join(All_genomes_sample, 'All_200k_tail_for_tokenizer_512bp.raw'))\n",
    "exons_raw_512 = os.path.abspath(os.path.join(All_genomes_sample, 'exons_512.raw'))\n",
    "introns_raw_512 = os.path.abspath(os.path.join(All_genomes_sample, 'introns_512.raw'))\n",
    "intergenic_raw_512 = os.path.abspath(os.path.join(All_genomes_sample, 'intergenic_512.raw'))\n",
    "\n",
    "name = \"All\"\n",
    "All_512 = os.path.abspath(os.path.join(All_genomes_tokenizer_folder, f'{name}_512/'))\n",
    "# All_1000 = os.path.abspath(os.path.join(All_genomes_tokenizer_folder, f'{name}_1000/'))\n",
    "name = \"Introns\"\n",
    "Introns_512 = os.path.abspath(os.path.join(All_genomes_tokenizer_folder, f'{name}_512/'))\n",
    "# Introns_1000 = os.path.abspath(os.path.join(All_genomes_tokenizer_folder, f'{name}_1000/'))\n",
    "name = \"Exons\"\n",
    "Exons_512 = os.path.abspath(os.path.join(All_genomes_tokenizer_folder, f'{name}_512/'))\n",
    "# Exons_1000 = os.path.abspath(os.path.join(All_genomes_tokenizer_folder, f'{name}_1000/'))\n",
    "name = \"Intergenic\"\n",
    "Intergenic_512 = os.path.abspath(os.path.join(All_genomes_tokenizer_folder, f'{name}_512/'))\n",
    "# Intergenic_1000 = os.path.abspath(os.path.join(All_genomes_tokenizer_folder, f'{name}_1000/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1fccb05b-99b1-49f7-b738-0a3d3dd0ef4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(All_genomes_sample_1000, \"r\") as file:\n",
    "#     print(file.readline())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7d6be0-db8d-41eb-a0d9-3ff069265421",
   "metadata": {
    "tags": []
   },
   "source": [
    "### All_512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f90a593d-6f2a-4869-af8d-41561a97dc9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer(vocabulary_size=5000, model=BPE, unk_token=<unk>, suffix=</w>, dropout=None, lowercase=False, unicode_normalizer=None, bert_normalizer=True, split_on_whitespace_only=False)\n",
      "Tokenizer(vocabulary_size=15000, model=BPE, unk_token=<unk>, suffix=</w>, dropout=None, lowercase=False, unicode_normalizer=None, bert_normalizer=True, split_on_whitespace_only=False)\n",
      "Tokenizer(vocabulary_size=50000, model=BPE, unk_token=<unk>, suffix=</w>, dropout=None, lowercase=False, unicode_normalizer=None, bert_normalizer=True, split_on_whitespace_only=False)\n"
     ]
    }
   ],
   "source": [
    "Filename = All_genomes_sample_512\n",
    "Folder = All_512\n",
    "Tokenizer = Tokenizer\n",
    "\n",
    "for Size in [5000, 15000, 50000]:\n",
    "    VocabSize = Size\n",
    "    get_tokenizer(Filename, Folder,  Tokenizer, VocabSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9cffa5f8-e224-4717-9d3c-1bb596a72ca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer(vocabulary_size=5000, model=BPE, unk_token=<unk>, suffix=</w>, dropout=None, lowercase=False, unicode_normalizer=None, bert_normalizer=True, split_on_whitespace_only=False)\n",
      "Tokenizer(vocabulary_size=15000, model=BPE, unk_token=<unk>, suffix=</w>, dropout=None, lowercase=False, unicode_normalizer=None, bert_normalizer=True, split_on_whitespace_only=False)\n",
      "Tokenizer(vocabulary_size=50000, model=BPE, unk_token=<unk>, suffix=</w>, dropout=None, lowercase=False, unicode_normalizer=None, bert_normalizer=True, split_on_whitespace_only=False)\n",
      "Tokenizer(vocabulary_size=5000, model=BPE, unk_token=<unk>, suffix=</w>, dropout=None, lowercase=False, unicode_normalizer=None, bert_normalizer=True, split_on_whitespace_only=False)\n",
      "Tokenizer(vocabulary_size=15000, model=BPE, unk_token=<unk>, suffix=</w>, dropout=None, lowercase=False, unicode_normalizer=None, bert_normalizer=True, split_on_whitespace_only=False)\n",
      "Tokenizer(vocabulary_size=50000, model=BPE, unk_token=<unk>, suffix=</w>, dropout=None, lowercase=False, unicode_normalizer=None, bert_normalizer=True, split_on_whitespace_only=False)\n",
      "Tokenizer(vocabulary_size=5000, model=BPE, unk_token=<unk>, suffix=</w>, dropout=None, lowercase=False, unicode_normalizer=None, bert_normalizer=True, split_on_whitespace_only=False)\n",
      "Tokenizer(vocabulary_size=15000, model=BPE, unk_token=<unk>, suffix=</w>, dropout=None, lowercase=False, unicode_normalizer=None, bert_normalizer=True, split_on_whitespace_only=False)\n",
      "Tokenizer(vocabulary_size=50000, model=BPE, unk_token=<unk>, suffix=</w>, dropout=None, lowercase=False, unicode_normalizer=None, bert_normalizer=True, split_on_whitespace_only=False)\n"
     ]
    }
   ],
   "source": [
    "Filenames = [exons_raw_512, introns_raw_512, intergenic_raw_512]\n",
    "Folders = [Exons_512, Introns_512, Intergenic_512]\n",
    "Tokenizer = Tokenizer\n",
    "\n",
    "for Filename, Folder in zip(Filenames, Folders):\n",
    "    for Size in [5000, 15000, 50000]:\n",
    "        VocabSize = Size\n",
    "        get_tokenizer(Filename, Folder,  Tokenizer, VocabSize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da638ea9-8788-410b-81e6-a0af22fddcbe",
   "metadata": {},
   "source": [
    "Try the limit (on incredibly high number 500 000 words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8f6e6290-d148-43ad-847d-dbebb970897e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer(vocabulary_size=500000, model=BPE, unk_token=<unk>, suffix=</w>, dropout=None, lowercase=False, unicode_normalizer=None, bert_normalizer=True, split_on_whitespace_only=False)\n"
     ]
    }
   ],
   "source": [
    "# Filename = All_genomes_sample_512\n",
    "# Folder = All_512\n",
    "# Tokenizer = Tokenizer\n",
    "\n",
    "# for Size in [500000]:\n",
    "#     VocabSize = Size\n",
    "#     get_tokenizer(Filename, Folder,  Tokenizer, VocabSize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8899965e-bf36-4a72-b113-210ec640e758",
   "metadata": {
    "tags": []
   },
   "source": [
    "### All_1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa977743-93f2-4116-9e42-84533c9eaa71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filename = All_genomes_sample_1000\n",
    "# Folder = All_1000\n",
    "# Tokenizer = Tokenizer\n",
    "\n",
    "# for Size in [5000, 15000, 50000]:\n",
    "#     VocabSize = Size\n",
    "#     get_tokenizer(Filename, Folder,  Tokenizer, VocabSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd36e19e-f0de-4caa-9b1d-2d3b6924c230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filenames = [exons_raw_1000, introns_raw_1000, intergenic_raw_1000]\n",
    "# Folders = [Exons_1000, Introns_1000, Intergenic_1000]\n",
    "# Tokenizer = Tokenizer\n",
    "\n",
    "# for Filename, Folder in zip(Filenames, Folders):\n",
    "#     for Size in [5000, 15000, 50000]:\n",
    "#         VocabSize = Size\n",
    "#         get_tokenizer(Filename, Folder,  Tokenizer, VocabSize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58be6db-b5fa-4e6a-9150-cad67adfc017",
   "metadata": {},
   "source": [
    "Try the limit (on incredibly high number 500 000 words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ddf00f10-2408-4cd8-95f7-1e19ab002cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer(vocabulary_size=500000, model=BPE, unk_token=<unk>, suffix=</w>, dropout=None, lowercase=False, unicode_normalizer=None, bert_normalizer=True, split_on_whitespace_only=False)\n"
     ]
    }
   ],
   "source": [
    "# Filename = All_genomes_sample_1000\n",
    "# Folder = All_1000\n",
    "# Tokenizer = Tokenizer\n",
    "\n",
    "# for Size in [500000]:\n",
    "#     VocabSize = Size\n",
    "#     get_tokenizer(Filename, Folder,  Tokenizer, VocabSize)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
