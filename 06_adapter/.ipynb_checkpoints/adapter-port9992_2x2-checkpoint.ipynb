{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9efd8465-9f12-47d5-9b58-56881800ca7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install adapter-transformers\n",
    "# !pip install torch\n",
    "# !pip install pandas\n",
    "# !pip install keras\n",
    "# !pip install datasets\n",
    "# !pip install tensorflow\n",
    "# !pip install sklearn\n",
    "# !pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e72cbde2-ea8c-4228-937e-34115b18c8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "data_path = \"/home/lieberze/DP/Thesis/model_training/data/512_bp_for_encoding/NEW/All_200k_tail_for_tokenizer.txt\"       \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbbe61dc-2f5b-4950-9863-41882fc3a7af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "file /home/lieberze/DP/Thesis/tokenizery_new_data/02_ByteLevelBPE/All_genomes_sample/All_512/5000/config.json not found\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://huggingface.co/docs/tokenizers/python/latest/api/reference.html#module-tokenizers.processors\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "path = \"/home/lieberze/DP/Thesis/tokenizery_new_data/02_ByteLevelBPE/All_genomes_sample/All_512/5000/\"\n",
    "tokenizer = ByteLevelBPETokenizer(\n",
    "    path + \"vocab.json\",\n",
    "    path + \"merges.txt\",\n",
    ")\n",
    "\n",
    "# https://huggingface.co/docs/transformers/preprocessing\n",
    "from tokenizers.processors import BertProcessing\n",
    "tokenizer._tokenizer.post_processor = BertProcessing(\n",
    "    (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    "    (\"<s>\", tokenizer.token_to_id(\"<s>\"))\n",
    ")\n",
    "\n",
    "tokenizer.save(\"byte-level-bpe.tokenizer.json\", pretty=True)\n",
    "\n",
    "from transformers import RobertaTokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained(path)\n",
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "raw",
   "id": "432bf577-176d-44e9-8e01-1ff4ae216147",
   "metadata": {},
   "source": [
    "vracet input_ids, attention_mask, labels (musi se to takhle jmenovat)\n",
    "dataset ma metodu set_format (columns, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab778e49-bd75-48e6-8568-dbc083c6a9d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({0: 'exon', 1: 'other'}, {'exon': 0, 'other': 1})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2label = {id:label for id, label in enumerate([\"exon\", \"other\"])}\n",
    "label2id = {label:id for id,label in id2label.items()}\n",
    "id2label, label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5b7bffc-3303-431f-9125-7d1d2287afc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = \"exon\"\n",
    "# label2id[a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74b9681a-fbd2-4d7c-87be-a00dd8dc7f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from datasets import load_dataset\n",
    "\n",
    "# defining the Dataset class\n",
    "class data_set(Dataset):\n",
    "    def __init__(self, data, labels, tokenizer):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        seq = self.data[index]\n",
    "        lab = self.labels[index]\n",
    "        lab_id = label2id[lab]\n",
    "        tokenized = tokenizer(seq, max_length=128, padding=\"max_length\", truncation=True)   \n",
    "        tokenized_with_label = tokenized\n",
    "        tokenized_with_label[\"labels\"] = lab_id\n",
    "        return tokenized_with_label\n",
    "    \n",
    "df = pd.read_csv(data_path, sep=\"\\t\", names=['type','sequence'])\n",
    "subs = {\n",
    "        \"exon\": \"exon\", \n",
    "        \"intron\": \"other\",\n",
    "        \"intergenic\": \"other\"\n",
    "        }\n",
    "y = [subs.get(item) for item in df[\"type\"]]\n",
    "df[\"type\"] = y\n",
    "dataset = data_set(df[\"sequence\"],df[\"type\"], tokenizer)\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True) #, collate_fn=collate_tokenize) #, collate_fn=lambda x: x )\n",
    "data = next(iter(dataloader))\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f4bc401-c82a-423b-a606-30840d574aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for epoch in range(2):\n",
    "#     print(\"\\n==============================\\n\")\n",
    "#     print(\"Epoch = \" + str(epoch))\n",
    "#     for (batch_idx, batch) in enumerate(dataloader):\n",
    "#         print(\"\\nBatch = \" + str(batch_idx))\n",
    "#         print(batch)\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc0b9d9a-ab70-4a53-af6c-ce2016879d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_train, df_eval = train_test_split(df, test_size=0.25, random_state=42, stratify=df[\"type\"])\n",
    "df_train, df_holdout = train_test_split(df_train, test_size=0.1, random_state=42, stratify=df_train[\"type\"])\n",
    "\n",
    "df_train = df_train.reset_index()\n",
    "df_eval = df_eval.reset_index()\n",
    "df_holdout = df_holdout.reset_index()\n",
    "\n",
    "data_set_train = data_set(df_train[\"sequence\"],df_train[\"type\"], tokenizer)\n",
    "data_set_eval = data_set(df_eval[\"sequence\"],df_eval[\"type\"], tokenizer)\n",
    "# nesahat :)\n",
    "holdout_test = data_set(df_holdout[\"sequence\"],df_holdout[\"type\"], tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "239c714d-b2c0-471e-a695-40e39eb4d889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_optimization_steps: 2109.375 \n",
      "how many 'eval_steps' to set: 211 \n",
      "how many steps in each evaluation stop: 130.20833333333334 \n",
      "hence in total: 27473.958333333336 steps for the whole evaluation\n"
     ]
    }
   ],
   "source": [
    "eval_size = len(df_eval)\n",
    "number_of_epochs = 6\n",
    "WANTED_eval_data_points_ratio = 0.1\n",
    "    # proportionally to the whole dataset size. e.g. 0.1 == 10% of all optimization\n",
    "    # steps is going to have an evaluation datapoint (loss)\n",
    "\n",
    "train_batch_size = eval_batch_size = 64\n",
    "total_optimization_steps = len(df_train)/train_batch_size\n",
    "eval_steps_in_one_run = eval_size/(number_of_epochs*train_batch_size)\n",
    "eval_steps_to_set=int(round(total_optimization_steps*WANTED_eval_data_points_ratio, 0))\n",
    "\n",
    "print(\"total_optimization_steps:\",total_optimization_steps,\n",
    "      \"\\nhow many 'eval_steps' to set:\",eval_steps_to_set,\n",
    "      \"\\nhow many steps in each evaluation stop:\",eval_steps_in_one_run,\n",
    "      \"\\nhence in total:\", eval_steps_to_set*eval_steps_in_one_run, \"steps for the whole evaluation\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "563baabc-0396-4295-88f1-fbead29fa756",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-trained-new-tokenizer_params_1 were not used when initializing RobertaModelWithHeads: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-trained-new-tokenizer_params_1 and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdapterTrainer, AutoModelWithHeads #TrainingsArguments\n",
    "from transformers.training_args import TrainingArguments\n",
    "# https://docs.adapterhub.ml/training.html\n",
    "# https://discuss.huggingface.co/t/keyerror-loss-while-training-qna/4111\n",
    "# https://huggingface.co/docs/transformers/main_classes/trainer\n",
    "\n",
    "model = AutoModelWithHeads.from_pretrained('roberta-trained-new-tokenizer_params_1')\n",
    "adapter_name = \"LR_3e-4_NEW_200k_new_tokenizer\"\n",
    "model.add_adapter(adapter_name)\n",
    "model.add_classification_head(adapter_name, num_labels=2, id2label = id2label) #, multilabel=False)\n",
    "model.train_adapter(adapter_name) # inicializace\n",
    "\n",
    "training_args =  TrainingArguments(\n",
    "    learning_rate=3e-4,\n",
    "    num_train_epochs=number_of_epochs,\n",
    "    report_to=\"wandb\",\n",
    "    output_dir = \"adapter_dir_NEW\",\n",
    "    label_names = [\"exon\", \"other\"],\n",
    "    eval_steps = eval_steps_to_set, \n",
    "    evaluation_strategy=\"steps\",\n",
    "    per_device_train_batch_size=train_batch_size,\n",
    "    per_device_eval_batch_size=eval_batch_size,\n",
    ")\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from transformers import EvalPrediction\n",
    "def compute_acc(p: EvalPrediction):\n",
    "    preds, labels = p\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\"acc\": acc}\n",
    "\n",
    "model.metrics=['accuracy'] #optimizer=opt, loss=loss,\n",
    "\n",
    "trainer = AdapterTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=data_set_train,\n",
    "        eval_dataset=data_set_eval,\n",
    "        compute_metrics = compute_acc,\n",
    "        # tokenizer=tokenizer, # data uz jsou ztokenizovana, netreba\n",
    "        # collator netreba\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d591da6e-02c4-4f10-808b-0fe04b3012fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch in trainer.get_train_dataloader():\n",
    "#     break\n",
    "# batch = {k: v.cuda() for k, v in batch.items()}\n",
    "# outputs = trainer.model(**batch)\n",
    "# batch, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "27280335-1fc9-4a2b-94e9-14712598aa34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 135000\n",
      "  Num Epochs = 6\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 192\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 4224\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mliebelife\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/liebelife/huggingface/runs/1wfe2zyj\" target=\"_blank\">adapter_dir_NEW</a></strong> to <a href=\"https://wandb.ai/liebelife/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/home/lieberze/.conda/envs/dp/lib/python3.6/site-packages/torch/nn/parallel/parallel_apply.py\", line 61, in _worker\n    output = module(*input, **kwargs)\n  File \"/home/lieberze/.conda/envs/dp/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/lieberze/.conda/envs/dp/lib/python3.6/site-packages/transformers/models/roberta/modeling_roberta.py\", line 964, in forward\n    adapter_names=adapter_names,\n  File \"/home/lieberze/.conda/envs/dp/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/lieberze/.conda/envs/dp/lib/python3.6/site-packages/transformers/models/roberta/modeling_roberta.py\", line 888, in forward\n    **kwargs,\n  File \"/home/lieberze/.conda/envs/dp/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/lieberze/.conda/envs/dp/lib/python3.6/site-packages/transformers/models/roberta/modeling_roberta.py\", line 550, in forward\n    **kwargs,\n  File \"/home/lieberze/.conda/envs/dp/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/lieberze/.conda/envs/dp/lib/python3.6/site-packages/transformers/models/roberta/modeling_roberta.py\", line 469, in forward\n    self.feed_forward_chunk, self.chunk_size_feed_forward, self.seq_len_dim, attention_output, **kwargs\n  File \"/home/lieberze/.conda/envs/dp/lib/python3.6/site-packages/transformers/modeling_utils.py\", line 2338, in apply_chunking_to_forward\n    return forward_fn(*input_tensors, **kwargs)\n  File \"/home/lieberze/.conda/envs/dp/lib/python3.6/site-packages/transformers/models/roberta/modeling_roberta.py\", line 480, in feed_forward_chunk\n    intermediate_output = self.intermediate(attention_output)\n  File \"/home/lieberze/.conda/envs/dp/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/lieberze/.conda/envs/dp/lib/python3.6/site-packages/transformers/models/roberta/modeling_roberta.py\", line 377, in forward\n    hidden_states = self.intermediate_act_fn(hidden_states)\n  File \"/home/lieberze/.conda/envs/dp/lib/python3.6/site-packages/torch/nn/functional.py\", line 1556, in gelu\n    return torch._C._nn.gelu(input)\nRuntimeError: CUDA out of memory. Tried to allocate 96.00 MiB (GPU 0; 10.76 GiB total capacity; 1.92 GiB already allocated; 20.56 MiB free; 1.93 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-3435b262f1ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/dp/lib/python3.6/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1289\u001b[0m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1290\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1291\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1293\u001b[0m                 if (\n",
      "\u001b[0;32m~/.conda/envs/dp/lib/python3.6/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   1822\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1823\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1824\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1825\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1826\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_gpu\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/dp/lib/python3.6/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   1854\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1855\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1856\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1857\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1858\u001b[0m         \u001b[0;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/dp/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/dp/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    166\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0mreplicas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/dp/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mparallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/dp/lib/python3.6/site-packages/torch/nn/parallel/parallel_apply.py\u001b[0m in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m             \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/dp/lib/python3.6/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0;31m# instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 434\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/home/lieberze/.conda/envs/dp/lib/python3.6/site-packages/torch/nn/parallel/parallel_apply.py\", line 61, in _worker\n    output = module(*input, **kwargs)\n  File \"/home/lieberze/.conda/envs/dp/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/lieberze/.conda/envs/dp/lib/python3.6/site-packages/transformers/models/roberta/modeling_roberta.py\", line 964, in forward\n    adapter_names=adapter_names,\n  File \"/home/lieberze/.conda/envs/dp/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/lieberze/.conda/envs/dp/lib/python3.6/site-packages/transformers/models/roberta/modeling_roberta.py\", line 888, in forward\n    **kwargs,\n  File \"/home/lieberze/.conda/envs/dp/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/lieberze/.conda/envs/dp/lib/python3.6/site-packages/transformers/models/roberta/modeling_roberta.py\", line 550, in forward\n    **kwargs,\n  File \"/home/lieberze/.conda/envs/dp/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/lieberze/.conda/envs/dp/lib/python3.6/site-packages/transformers/models/roberta/modeling_roberta.py\", line 469, in forward\n    self.feed_forward_chunk, self.chunk_size_feed_forward, self.seq_len_dim, attention_output, **kwargs\n  File \"/home/lieberze/.conda/envs/dp/lib/python3.6/site-packages/transformers/modeling_utils.py\", line 2338, in apply_chunking_to_forward\n    return forward_fn(*input_tensors, **kwargs)\n  File \"/home/lieberze/.conda/envs/dp/lib/python3.6/site-packages/transformers/models/roberta/modeling_roberta.py\", line 480, in feed_forward_chunk\n    intermediate_output = self.intermediate(attention_output)\n  File \"/home/lieberze/.conda/envs/dp/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/lieberze/.conda/envs/dp/lib/python3.6/site-packages/transformers/models/roberta/modeling_roberta.py\", line 377, in forward\n    hidden_states = self.intermediate_act_fn(hidden_states)\n  File \"/home/lieberze/.conda/envs/dp/lib/python3.6/site-packages/torch/nn/functional.py\", line 1556, in gelu\n    return torch._C._nn.gelu(input)\nRuntimeError: CUDA out of memory. Tried to allocate 96.00 MiB (GPU 0; 10.76 GiB total capacity; 1.92 GiB already allocated; 20.56 MiB free; 1.93 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d0011f-34a0-4d1a-a8c2-b4f620a3dedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"adapter-sequence-types-NEW\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8964565a-8722-425c-b437-51cfc11b3071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_adapter(\"./adapter-sequence-types/\", adapter_name) # jen adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec998f08-617d-4313-a49a-61ff76195d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pak mu dam holdout a udelam confusion matrix, model.predict. vyvazeny data\n",
    "# list s predikcema, true values a pak conf_matice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649321af-0340-438b-ba1a-6d2570fbcd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelWithHeads.from_pretrained(\"roberta-trained-new-tokenizer_params_1\")\n",
    "tokenizer = tokenizer\n",
    "\n",
    "adapter1 = model.load_adapter(\"adapter-sequence-types-NEW/LR_3e-4_NEW_200k_new_tokenizer\")\n",
    "\n",
    "# model.active_adapters = ac.Parallel(adapter1)\n",
    "model.active_adapters = adapter1\n",
    "\n",
    "# input_ids = tokenizer(a, return_tensors=\"pt\")\n",
    "# print(\"STS-B adapter output:\", output1)\n",
    "# print(\"MRPC adapter output:\", bool(torch.argmax(output1[0]).item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9cb55a9-787d-4168-bd67-f5d6b8215caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757ba8e7-fc61-46b1-b283-e9b62a049fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import transformers.adapters.composition as ac\n",
    "# model = AutoModelWithHeads.from_pretrained('roberta-trained')\n",
    "# model.add_adapter(\"adapter-sequence-types\")\n",
    "# model.active_adapters = ac.Stack(\"adapter-sequence-types\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7144c8-a035-48d1-88a7-2571b3fc02bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.to(\"cuda:0\")\n",
    "# trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3b96a0-7d22-4866-8e39-e40a9aae0fe9",
   "metadata": {},
   "source": [
    "training_args.device.indexhttps://github.com/NielsRogge/Transformers-Tutorials/blob/master/VisionTransformer/Fine_tuning_the_Vision_Transformer_on_CIFAR_10_with_the_%F0%9F%A4%97_Trainer.ipynb\n",
    "\n",
    "hlavne konec - evaluace a confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e62093f-387a-4902-8a60-fa3fc5fdb92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextClassificationPipeline\n",
    "classifier = TextClassificationPipeline(model=model, tokenizer=tokenizer, device=training_args.device.index)\n",
    "# classifier(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94aac40-f4cf-4c89-af06-2200e9282e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.to(torch.device(\"cuda:0\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fad48f-77f9-4e4a-8750-8d95d92a8837",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = list(df_holdout.sequence)\n",
    "true_labels = list(df_holdout.type)\n",
    "pred_labels = classifier(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16b3268-55b8-48fc-a2e2-7937083fef18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_labels = []\n",
    "# for sequence in sequences:\n",
    "#     pred_label = classifier(sequence)\n",
    "#     pred_labels.append(pred_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1043fa-9c8e-4631-b88e-c75a7a4b388b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_lab = [i[\"label\"] for i in pred_labels]\n",
    "pred_score = [i[\"score\"] for i in pred_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7b40f4-6de6-49a8-b08f-5085da924c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_labels[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f7e909-909b-4dc1-a6d0-0f6b41d9ebf7",
   "metadata": {},
   "source": [
    "tady je videt, ze jakmile je to exon, tak si je hodne jistej => nejspis se uci podle delky :/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9ba0f6-c69e-46fb-8294-f777e5693d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab91ecc-fd5d-406c-b3f3-1cef3b7a7e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770ba66d-fdf1-4e29-8690-314a37932e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = metrics.confusion_matrix(true_labels, pred_lab, normalize='true')\n",
    "\n",
    "# confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "df_cm = pd.DataFrame(cm)\n",
    "df_cm.columns = ['exon', 'other']\n",
    "df_cm.index = ['exon', 'other']\n",
    "plt.title('Confusion Matrix, normalized', size=16)\n",
    "sns.heatmap(df_cm, annot=True, cmap='Blues')\n",
    "\n",
    "plt.savefig('200k-NEW-all_yellow.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fad4bf6-644f-4ac3-8dae-aceb29c32da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "report = metrics.classification_report(true_labels, pred_lab, digits=2, output_dict=True, zero_division=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c83854-7543-4c30-8718-05db7a27a874",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_report = pd.DataFrame(report).transpose()\n",
    "df_report\n",
    "# SHOW THE DIFFERENCES\n",
    "# exon, intron, intergenic = df_report[\"f1-score\"].exon, df_report[\"f1-score\"].other\n",
    "# exon_vs_rest_ratio = exon/(intron + intergenic)\n",
    "# exon_vs_rest_f1_score = exon, intron + intergenic, exon_vs_rest_ratio\n",
    "print(df_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffd6cba-1bd1-41ed-8361-dc95b8d1bfb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exon_vs_rest_f1_score # (exon, intron+intergenic, their ratio)\n",
    "# print(\"ratio of exon vs rest success:\", exon_vs_rest_f1_score[2]) # zamerujeme se na uspech predikce exonu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c973efd3-0d36-4822-99bb-2073646e8091",
   "metadata": {},
   "source": [
    "u nestejne dlouhych - horsi nez ML modely = 0.77 (mely 0.81 az 0.83), ale ted, po uprave delek je to lepsi 0.86 (zkouseno na 20k vzorku)))))\n",
    "\n",
    "0.72 pro natrenovane na kratkych, ale adapter na 100k dlouhych\n",
    "0.77 pro natrenovane na kratkych a adapter taky na nich"
   ]
  },
  {
   "cell_type": "raw",
   "id": "dfed34c9-9272-417b-bf9f-86d61d1a985a",
   "metadata": {},
   "source": [
    "or"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b3b596-e6fb-4e80-b91f-de92dde0491c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.login()\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66ab65e-50b8-4d1b-824a-58182978e418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputs = trainer.predict(test_ds) # test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d82e0e3-a28e-4939-bf3e-496cdf056cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# y_true = outputs.label_ids\n",
    "# y_pred = outputs.predictions.argmax(1)\n",
    "\n",
    "# labels = train_ds.features['label'].names\n",
    "# cm = confusion_matrix(y_true, y_pred)\n",
    "# disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "# disp.plot(xticks_rotation=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eddb541-f524-44b3-935f-ea3d854fbe56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the correct way to predict with a trained model is prediction = model(tokenized_sequence_to_classify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cb4484-f7c1-4544-ba44-de40a07b9b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output1 = model(**input_ids)\n",
    "# torch.argmax(output1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ced4455-a42c-488e-a90a-e1016075926f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_adapter('roberta-trained', \"./adapter-sequence-types/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae3902d-ab68-4552-a779-40e7e13fdbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import transformers.adapters.composition as ac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e9e939-97d2-4a75-bd7e-e741566899f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model.load_adapter(\"adapter-sequence-types\")\n",
    "# model.set_active_adapters(\"adapter-sequence-types\")\n",
    "# model.predict() # pozor! asi potreba male batche, test set (holdout)\n",
    "# asi bude potreba pushnout data na gpu rucne:\n",
    "# model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9eeb6cf-3ac6-4d09-8c43-676e21836e24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# #initializing the model\n",
    "# model = MLP().to(device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
