{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4d960ba-6d10-4bf0-9468-a60a396e8b5c",
   "metadata": {},
   "source": [
    "# Optimization for adapter batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fb0c55e9-f3a9-4ece-90ba-bdcb321192b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "suffix = \"3x3\" # multilabel classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9efd8465-9f12-47d5-9b58-56881800ca7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install adapter-transformers\n",
    "# !pip install torch\n",
    "# !pip install pandas\n",
    "# !pip install keras\n",
    "# !pip install datasets\n",
    "# !pip install tensorflow\n",
    "# !pip install sklearn\n",
    "# !pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e72cbde2-ea8c-4228-937e-34115b18c8d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "file /home/lieberze/DP/Thesis/02_tokenizery_new_data/02_ByteLevelBPE/All_genomes_sample/All_512/5000/config.json not found\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import pickle\n",
    "data_path = \"/home/lieberze/DP/Thesis/05_model_training/data/512_bp_for_encoding/NEW/All_100k.txt\"       \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# https://huggingface.co/docs/tokenizers/python/latest/api/reference.html#module-tokenizers.processors\n",
    "path = \"/home/lieberze/DP/Thesis/02_tokenizery_new_data/02_ByteLevelBPE/All_genomes_sample/All_512/5000/\"\n",
    "\n",
    "from transformers import RobertaTokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained(path)\n",
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "90849fff-90f6-4f6f-aab2-bc09d8e0a259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PreTrainedTokenizer(name_or_path='/home/lieberze/DP/Thesis/02_tokenizery_new_data/02_ByteLevelBPE/All_genomes_sample/All_512/5000/', vocab_size=5000, model_max_len=1000000000000000019884624838656, is_fast=False, padding_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'sep_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'pad_token': AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'cls_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True)})\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ab778e49-bd75-48e6-8568-dbc083c6a9d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({0: 'exon', 1: 'intron', 2: 'intergenic'},\n",
       " {'exon': 0, 'intron': 1, 'intergenic': 2})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2label = {id:label for id, label in enumerate([\"exon\", \"intron\", \"intergenic\"])}\n",
    "label2id = {label:id for id,label in id2label.items()}\n",
    "id2label, label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "74b9681a-fbd2-4d7c-87be-a00dd8dc7f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from datasets import load_dataset\n",
    "\n",
    "# defining the Dataset class\n",
    "# there is also a method set_format (columns, ...)\n",
    "class data_set(Dataset):\n",
    "    def __init__(self, data, labels, tokenizer):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        seq = self.data[index]\n",
    "        lab = self.labels[index]\n",
    "        lab_id = label2id[lab]\n",
    "        tokenized = tokenizer(seq, max_length=128, padding=\"max_length\", truncation=True)   \n",
    "        tokenized_with_label = tokenized\n",
    "        tokenized_with_label[\"labels\"] = lab_id # possible arguments are: input_ids, attention_mask, labels\n",
    "        return tokenized_with_label\n",
    "    \n",
    "df = pd.read_csv(data_path, sep=\"\\t\", names=['type','sequence'])\n",
    "dataset = data_set(df[\"sequence\"],df[\"type\"], tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dc0b9d9a-ab70-4a53-af6c-ce2016879d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_train, df_eval = train_test_split(df, test_size=0.15, random_state=42, stratify=df[\"type\"])\n",
    "df_train, df_holdout = train_test_split(df_train, test_size=0.1, random_state=42, stratify=df_train[\"type\"])\n",
    "\n",
    "df_train = df_train.reset_index()\n",
    "df_eval = df_eval.reset_index()\n",
    "df_holdout = df_holdout.reset_index()\n",
    "\n",
    "data_set_train = data_set(df_train[\"sequence\"],df_train[\"type\"], tokenizer)\n",
    "data_set_eval = data_set(df_eval[\"sequence\"],df_eval[\"type\"], tokenizer)\n",
    "# don't touch :)\n",
    "holdout_test = data_set(df_holdout[\"sequence\"],df_holdout[\"type\"], tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "25982331-faea-4086-be6f-67cf55812566",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "Transformers = \"/home/lieberze/DP/Thesis/05_model_training/\"\n",
    "TransformerName = os.path.abspath(os.path.join(Transformers, \"roberta-trained-new-tokenizer_params_4\")) \n",
    "# the correct way to predict with a trained model is prediction = model(tokenized_sequence_to_classify)\n",
    "\n",
    "SaveToFolder = \"adapter_optimize\"\n",
    "LabelNames = [\"exon\", \"intron\", \"intergenic\"]\n",
    "num_labels = len(LabelNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "563baabc-0396-4295-88f1-fbead29fa756",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdapterTrainer, AutoModelWithHeads #TrainingsArguments\n",
    "from transformers.training_args import TrainingArguments\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import wandb\n",
    "    # https://docs.adapterhub.ml/training.html\n",
    "    # https://discuss.huggingface.co/t/keyerror-loss-while-training-qna/4111\n",
    "    # https://huggingface.co/docs/transformers/main_classes/trainer\n",
    "    \n",
    "number_of_epochs = 5\n",
    "num_GPU = 3\n",
    "WANTED_eval_data_points_ratio = 0.1\n",
    "\n",
    "def TrainAdapter(ModelName, SaveToFolder, AdapterName, LR, WR, AE, TBS):\n",
    "    wandb.init(project=\"adapter_optimize\")\n",
    "               \n",
    "    model = AutoModelWithHeads.from_pretrained(ModelName)\n",
    "    adapter_name = AdapterName\n",
    "    model.add_adapter(adapter_name)\n",
    "    model.add_classification_head(adapter_name, num_labels=num_labels, id2label = id2label) #, multilabel=False)\n",
    "    model.train_adapter(adapter_name) # inicializace\n",
    "    \n",
    "    # train_batch_size = eval_batch_size = 64\n",
    "    total_optimization_steps = round(len(df_train)/TBS/num_GPU)*number_of_epochs\n",
    "    eval_steps_in_one_run = round(len(df_eval)/(number_of_epochs*TBS)/num_GPU) \n",
    "    eval_steps_to_set=int(round(total_optimization_steps*WANTED_eval_data_points_ratio, 0))\n",
    "    print(\"total_optimization_steps:\",total_optimization_steps,\n",
    "          \"\\nhow many 'eval_steps' to set:\",eval_steps_to_set,\n",
    "          \"\\nhow many steps in each evaluation stop:\",eval_steps_in_one_run,\n",
    "          \"\\nhence in total:\", eval_steps_to_set*eval_steps_in_one_run, \"steps for the whole evaluation\" )\n",
    "\n",
    "    training_args =  TrainingArguments(\n",
    "        learning_rate=LR,\n",
    "        num_train_epochs=number_of_epochs,\n",
    "        report_to=\"wandb\",\n",
    "        output_dir = SaveToFolder,\n",
    "        label_names = LabelNames,\n",
    "        eval_steps = eval_steps_to_set, \n",
    "        evaluation_strategy=\"steps\",\n",
    "        per_device_train_batch_size=TBS, # poladit (32, 64, 128)\n",
    "        per_device_eval_batch_size=TBS,\n",
    "        do_eval=True,\n",
    "        logging_steps=total_optimization_steps*0.05, \n",
    "        warmup_ratio = WR,\n",
    "        adam_epsilon=AE,\n",
    "        weight_decay=0.0005,\n",
    "        save_steps=10_000,\n",
    "        save_total_limit=2,\n",
    "        # prediction_loss_only # rozhodne ne!\n",
    "         # The next line is important to ensure the dataset labels are properly passed to the model\n",
    "        remove_unused_columns=False,\n",
    "        seed=1,\n",
    "    )\n",
    "\n",
    "    from sklearn import metrics\n",
    "    from sklearn.metrics import accuracy_score, f1_score\n",
    "    from transformers import EvalPrediction\n",
    "    \n",
    "    def compute_metrics(p: EvalPrediction):\n",
    "        logits, labels = p\n",
    "        preds = np.argmax(logits, axis=-1)\n",
    "        acc = accuracy_score(labels, preds)\n",
    "        f1_weighted = f1_score(labels, preds, average='weighted')\n",
    "        \n",
    "        report = metrics.classification_report(labels, preds, digits=2, output_dict=True, zero_division=0)\n",
    "        df_report = pd.DataFrame(report).transpose()\n",
    "        return {\"acc\": acc, \"f1_weighted\": f1_weighted, \"report\":df_report}\n",
    "\n",
    "    model.metrics = ['f1_weighted'] #optimizer=opt, loss=loss,\n",
    "\n",
    "    trainer = AdapterTrainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=data_set_train,\n",
    "            eval_dataset=data_set_eval,\n",
    "            compute_metrics = compute_metrics,\n",
    "        )\n",
    "    \n",
    "    # first train model, save results\n",
    "    result = trainer.train()\n",
    "    \n",
    "    # evaluation metrics, manually, because (for unknown reason) the model itself doesn't return them\n",
    "    y_pr = data_set_eval.labels\n",
    "    y_pr = [label2id[i] for i in y_pr]\n",
    "    loss = trainer.compute_metrics([trainer.predict(data_set_eval).predictions[1], y_pr])\n",
    "    Metrics = trainer.evaluate()\n",
    "    \n",
    "    print(loss, Metrics)\n",
    "    !mkdir -p $SaveToFolder/$AdapterName\n",
    "    \n",
    "    # saving results\n",
    "    with open(f\"{SaveToFolder}/{AdapterName}/metrics_{suffix}.pkl\", \"wb\") as f:\n",
    "        pickle.dump({\"loss\":loss, \"metrics\":Metrics, \"result\":result}, f)\n",
    "\n",
    "    trainer.save_model(SaveToFolder)\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58df6c35-d328-469f-a127-888a5785b6fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mliebelife\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.11 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/liebelife/adapter_optimize/runs/3td2ata4\" target=\"_blank\">fanciful-river-49</a></strong> to <a href=\"https://wandb.ai/liebelife/adapter_optimize\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/lieberze/DP/Thesis/05_model_training/roberta-trained-new-tokenizer_params_4 were not used when initializing RobertaModelWithHeads: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at /home/lieberze/DP/Thesis/05_model_training/roberta-trained-new-tokenizer_params_4 and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_optimization_steps: 2215 \n",
      "how many 'eval_steps' to set: 222 \n",
      "how many steps in each evaluation stop: 16 \n",
      "hence in total: 3552 steps for the whole evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 85000\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 192\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2215\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "/home/lieberze/.conda/envs/dp/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='73' max='2215' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  73/2215 06:16 < 3:09:24, 0.19 it/s, Epoch 0.16/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TRY:\n",
    "choices = [[0.0003, 0.08, 1e-08, 4],\n",
    "           [0.0003, 0.08, 1e-08, 16],\n",
    "           [0.0003, 0.05, 1e-08, 32],\n",
    "           [0.0003, 0.05, 1e-08, 64],]\n",
    "            # [0.0003, 0.05, 1e-08, 128]] # killed job (too big for memory)\n",
    "\n",
    "for i,choice in enumerate(choices):\n",
    "    LR, WR, AE, TBS = choice\n",
    "    TrainAdapter(TransformerName, SaveToFolder, f\"adapter_batchsize_{TBS}\", LR, WR, AE, TBS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "68c51370-2fd3-4aa6-a78d-2ee4e05468a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 16, 32, 64]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_sizes = [i[-1] for i in choices]\n",
    "batch_sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ef9f7f-ddc4-40e7-b2fb-a78381af8850",
   "metadata": {},
   "source": [
    "This may or may not be necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9233d6d0-007e-49b9-aaf8-7b39baeab3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_adapter(\"./adapter-sequence-types/\", adapter_name) # saving only the adapter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7655ff39-4384-4819-a478-93df690b4362",
   "metadata": {},
   "source": [
    "## Loading the adapter + transformer and prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c1591c-0a6a-4a68-82c9-bbf5828d5f7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing batchsize 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/lieberze/DP/Thesis/05_model_training/roberta-trained-new-tokenizer_params_4 were not used when initializing RobertaModelWithHeads: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at /home/lieberze/DP/Thesis/05_model_training/roberta-trained-new-tokenizer_params_4 and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/lieberze/.conda/envs/dp/lib/python3.6/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'intron', 'score': 0.43355241417884827}, {'label': 'intron', 'score': 0.4161914587020874}, {'label': 'exon', 'score': 0.9289034008979797}, {'label': 'intergenic', 'score': 0.37669631838798523}, {'label': 'intergenic', 'score': 0.4883517920970917}, {'label': 'intron', 'score': 0.5470800399780273}, {'label': 'intron', 'score': 0.4596070349216461}, {'label': 'intergenic', 'score': 0.5114262700080872}, {'label': 'intron', 'score': 0.4685747027397156}, {'label': 'intron', 'score': 0.41981571912765503}, {'label': 'intergenic', 'score': 0.4568425118923187}, {'label': 'exon', 'score': 0.3957570791244507}, {'label': 'exon', 'score': 0.6212294101715088}, {'label': 'exon', 'score': 0.9940525889396667}, {'label': 'intron', 'score': 0.520086407661438}, {'label': 'intergenic', 'score': 0.5287533402442932}, {'label': 'intron', 'score': 0.4231276512145996}, {'label': 'exon', 'score': 0.45736563205718994}, {'label': 'intergenic', 'score': 0.41922834515571594}, {'label': 'intron', 'score': 0.5249698162078857}, {'label': 'intergenic', 'score': 0.5368358492851257}, {'label': 'intergenic', 'score': 0.6555995941162109}, {'label': 'intron', 'score': 0.43602171540260315}, {'label': 'intergenic', 'score': 0.5194985270500183}, {'label': 'intron', 'score': 0.4750918745994568}, {'label': 'intergenic', 'score': 0.450117826461792}, {'label': 'exon', 'score': 0.7646355628967285}, {'label': 'intergenic', 'score': 0.40973496437072754}, {'label': 'exon', 'score': 0.9859933853149414}, {'label': 'intergenic', 'score': 0.621997594833374}, {'label': 'intergenic', 'score': 0.5121273398399353}, {'label': 'intron', 'score': 0.4771767556667328}, {'label': 'intron', 'score': 0.5056489706039429}, {'label': 'exon', 'score': 0.995914876461029}, {'label': 'intron', 'score': 0.3575226366519928}, {'label': 'intron', 'score': 0.48314082622528076}, {'label': 'exon', 'score': 0.9937257170677185}, {'label': 'intron', 'score': 0.4492807984352112}, {'label': 'exon', 'score': 0.5530699491500854}, {'label': 'exon', 'score': 0.9218155145645142}, {'label': 'intron', 'score': 0.46251609921455383}, {'label': 'intergenic', 'score': 0.6003195643424988}, {'label': 'intergenic', 'score': 0.6433302164077759}, {'label': 'intergenic', 'score': 0.4529775083065033}, {'label': 'exon', 'score': 0.3781854510307312}, {'label': 'exon', 'score': 0.8826794028282166}, {'label': 'intergenic', 'score': 0.5946040153503418}, {'label': 'intron', 'score': 0.4130994379520416}, {'label': 'intergenic', 'score': 0.48653197288513184}, {'label': 'intergenic', 'score': 0.6305356025695801}]\n",
      "processing batchsize 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/lieberze/DP/Thesis/05_model_training/roberta-trained-new-tokenizer_params_4 were not used when initializing RobertaModelWithHeads: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at /home/lieberze/DP/Thesis/05_model_training/roberta-trained-new-tokenizer_params_4 and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/lieberze/.conda/envs/dp/lib/python3.6/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n"
     ]
    }
   ],
   "source": [
    "from transformers import TextClassificationPipeline\n",
    "import pickle\n",
    "from sklearn import metrics\n",
    "\n",
    "for batchsize in batch_sizes[]:\n",
    "    print(f\"processing batchsize {batchsize}\")\n",
    "    adapter_batchsize = batchsize\n",
    "    model = AutoModelWithHeads.from_pretrained(TransformerName)\n",
    "    adapter1 = model.load_adapter(f\"{SaveToFolder}/adapter_batchsize_{adapter_batchsize}\")\n",
    "    model.active_adapters = adapter1\n",
    "    \n",
    "    sequences = list(df_holdout.sequence)\n",
    "    true_labels = list(df_holdout.type)\n",
    "\n",
    "    classifier = TextClassificationPipeline(model=model, tokenizer=tokenizer)#, device=training_args.device.index)\n",
    "\n",
    "    ## this takes a lot of time to compute so it is better to save the output for future use\n",
    "    pred_labels = classifier(sequences)\n",
    "    with open(f\"{SaveToFolder}/adapter_batchsize_{adapter_batchsize}/pred_labels_{suffix}.pk\", 'wb') as f:\n",
    "        pickle.dump(pred_labels, f)\n",
    "\n",
    "    pred_lab = [i[\"label\"] for i in pred_labels]\n",
    "    pred_score = [i[\"score\"] for i in pred_labels]\n",
    "\n",
    "    print(pred_labels[:50])\n",
    "    \n",
    "    report = metrics.classification_report(true_labels, pred_lab, digits=2, output_dict=True, zero_division=0)\n",
    "    df_report = pd.DataFrame(report)\n",
    "    df_report.to_pickle(f\"{SaveToFolder}/adapter_batchsize_{adapter_batchsize}/report_{suffix}.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a44e1cc-d43a-4b6b-8f0c-571e72133604",
   "metadata": {},
   "source": [
    "## Load all classification reports for batch optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c7a0b279-5797-4f18-a831-87e7a0aa0524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adapter with batch size: 4\n",
      "                  exon   intergenic       intron  accuracy    macro avg  \\\n",
      "precision     0.760353     0.522990     0.476809  0.581176     0.586717   \n",
      "recall        0.696278     0.593118     0.453583  0.581176     0.580993   \n",
      "f1-score      0.726906     0.555851     0.464906  0.581176     0.582554   \n",
      "support    2848.000000  2819.000000  2833.000000  0.581176  8500.000000   \n",
      "\n",
      "           weighted avg  \n",
      "precision      0.587129  \n",
      "recall         0.581176  \n",
      "f1-score       0.582853  \n",
      "support     8500.000000  \n",
      "adapter with batch size: 16\n",
      "                  exon   intergenic       intron  accuracy    macro avg  \\\n",
      "precision     0.757104     0.518725     0.457915  0.570118     0.577915   \n",
      "recall        0.682935     0.560128     0.466643  0.570118     0.569902   \n",
      "f1-score      0.718110     0.538632     0.462238  0.570118     0.572993   \n",
      "support    2848.000000  2819.000000  2833.000000  0.570118  8500.000000   \n",
      "\n",
      "           weighted avg  \n",
      "precision      0.578328  \n",
      "recall         0.570118  \n",
      "f1-score       0.573306  \n",
      "support     8500.000000  \n",
      "adapter with batch size: 32\n",
      "                  exon   intergenic       intron  accuracy    macro avg  \\\n",
      "precision     0.747656     0.503137     0.455843  0.562353     0.568879   \n",
      "recall        0.672051     0.597375     0.417226  0.562353     0.562217   \n",
      "f1-score      0.707840     0.546221     0.435680  0.562353     0.563247   \n",
      "support    2848.000000  2819.000000  2833.000000  0.562353  8500.000000   \n",
      "\n",
      "           weighted avg  \n",
      "precision      0.569302  \n",
      "recall         0.562353  \n",
      "f1-score       0.563530  \n",
      "support     8500.000000  \n",
      "adapter with batch size: 64\n",
      "                  exon   intergenic       intron  accuracy    macro avg  \\\n",
      "precision     0.744122     0.504947     0.449889  0.559176     0.566320   \n",
      "recall        0.666784     0.579283     0.430992  0.559176     0.559020   \n",
      "f1-score      0.703333     0.539567     0.440238  0.559176     0.561046   \n",
      "support    2848.000000  2819.000000  2833.000000  0.559176  8500.000000   \n",
      "\n",
      "           weighted avg  \n",
      "precision      0.566735  \n",
      "recall         0.559176  \n",
      "f1-score       0.561333  \n",
      "support     8500.000000  \n"
     ]
    }
   ],
   "source": [
    "for batchsize in batch_sizes:\n",
    "    print(f\"adapter with batch size: {batchsize}\")\n",
    "    with open(f\"{SaveToFolder}/adapter_batchsize_{batchsize}/report_{suffix}.pkl\", 'rb') as f:\n",
    "        x = pickle.load(f)\n",
    "    print(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
