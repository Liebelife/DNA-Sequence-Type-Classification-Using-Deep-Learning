loading configuration file /home/lieberze/DP/Thesis/05_model_training/roberta-trained-new-tokenizer_/config.json
Model config RobertaConfig {
  "adapters": {
    "adapters": {},
    "config_map": {},
    "fusion_config_map": {},
    "fusions": {}
  },
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "torch_dtype": "float32",
  "transformers_version": "4.11.3",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 5000
}
loading weights file /home/lieberze/DP/Thesis/05_model_training/roberta-trained-new-tokenizer_/pytorch_model.bin
Some weights of the model checkpoint at /home/lieberze/DP/Thesis/05_model_training/roberta-trained-new-tokenizer_ were not used when initializing RobertaModelWithHeads: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias']
- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at /home/lieberze/DP/Thesis/05_model_training/roberta-trained-new-tokenizer_ and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Adding adapter '3x3_fold_0'.
Adding head '3x3_fold_0' with config {'head_type': 'classification', 'num_labels': 3, 'layers': 2, 'activation_function': 'tanh', 'label2id': {'exon': 0, 'intergenic': 1, 'intron': 2}, 'use_pooler': False, 'bias': True}.
PyTorch: setting up devices
***** Running Prediction *****
  Num examples = 100
  Batch size = 48
/home/lieberze/.conda/envs/dp/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
total_optimization_steps: 19
how many 'eval_steps' to set: 2
how many steps in each evaluation stop: 2
hence in total: 4 steps for the whole evaluation
PredictionOutput(predictions=(array([1.1703252, 1.144701 , 1.084324 , 1.1468896, 1.1564194, 1.1579051,
       1.08526  , 1.1936088], dtype=float32), array([[ 0.10472833,  0.43514076,  0.03696492],
       [ 0.08501166,  0.2750226 ,  0.07979693],
       [ 0.09794366,  0.43291903,  0.05204891],
       [ 0.0733008 ,  0.37779847,  0.02910803],
       [ 0.08565147,  0.43396428,  0.01061874],
       [ 0.19757621,  0.25839424, -0.01091366],
       [-0.02038259,  0.42436492, -0.04030493],
       [ 0.11774729,  0.3592992 ,  0.06906304],
       [ 0.08256149,  0.42277962,  0.09189002],
       [-0.00213525,  0.2500801 , -0.03888397],
       [ 0.04522266,  0.37787613,  0.04276691],
       [ 0.08868779,  0.39878964,  0.03499802],
       [ 0.09515699,  0.44715935,  0.04541237],
       [ 0.07699198,  0.32441062,  0.07496055],
       [ 0.00855196,  0.37743548, -0.04784675],
       [ 0.0716425 ,  0.34112155, -0.0121538 ],
       [ 0.20317158,  0.27743077,  0.12703845],
       [ 0.07829419,  0.18399568,  0.11378007],
       [ 0.10391036,  0.36744383, -0.00533952],
       [ 0.03784898,  0.25129098, -0.03270444],
       [ 0.13228427,  0.39834362,  0.04597696],
       [ 0.08994912,  0.27184618,  0.03456829],
       [ 0.13868847,  0.35981703,  0.05513566],
       [ 0.13242213,  0.3689819 ,  0.05058083],
       [ 0.12718078,  0.32429242, -0.02540758],
       [ 0.15038438,  0.32847923,  0.06307964],
       [ 0.20692499,  0.3242736 ,  0.08865538],
       [ 0.06315441,  0.5388844 ,  0.1357711 ],
       [ 0.11521365,  0.24837503,  0.04567846],
       [ 0.13414668,  0.34798604,  0.01984501],
       [ 0.10970882,  0.30848664,  0.11336895],
       [ 0.07939632,  0.42788333,  0.02940538],
       [ 0.03416324,  0.254234  , -0.01255906],
       [ 0.08050262,  0.2945975 ,  0.01053379],
       [ 0.16800687,  0.29197326, -0.12158876],
       [ 0.13891427,  0.39607418,  0.01223591],
       [ 0.16338632,  0.24635637,  0.07614978],
       [ 0.11317734,  0.4352416 ,  0.13407455],
       [ 0.16090927,  0.29260838,  0.0538617 ],
       [ 0.19547419,  0.3137629 ,  0.08001528],
       [ 0.03196308,  0.20098636,  0.02299454],
       [ 0.12028979,  0.3326565 ,  0.08035207],
       [ 0.13306913,  0.38419765,  0.08466862],
       [ 0.10172817,  0.24355866,  0.04201352],
       [ 0.07189663,  0.3861026 ,  0.04820494],
       [ 0.06814158,  0.2751068 , -0.03886092],
       [ 0.07158258,  0.4720895 ,  0.16386908],
       [-0.11976187,  0.36884695, -0.08261476],
       [-0.00702987,  0.02543695,  0.15242484],
       [ 0.08280624,  0.35803497,  0.02074097],
       [ 0.19644283,  0.3801909 ,  0.10690202],
       [ 0.07239412,  0.3885059 ,  0.02996139],
       [ 0.115686  ,  0.37433568,  0.06763884],
       [ 0.10269844,  0.4137895 ,  0.06937979],
       [ 0.1818952 ,  0.32717407,  0.15523562],
       [ 0.06152003,  0.40045285,  0.06732301],
       [ 0.11678871,  0.4199152 ,  0.00727948],
       [ 0.06104065,  0.37105674,  0.02534922],
       [ 0.05453143,  0.38372362,  0.05007137],
       [-0.06309659,  0.35970968, -0.05008434],
       [ 0.17982984,  0.27271682,  0.04828618],
       [ 0.1068079 ,  0.4570948 ,  0.08035535],
       [ 0.11024348,  0.38645428,  0.08832083],
       [ 0.10455804,  0.40711075,  0.06719701],
       [ 0.07138842,  0.4337461 ,  0.04550529],
       [ 0.11481202,  0.38852054,  0.03453467],
       [ 0.11882576,  0.3192937 ,  0.10359994],
       [ 0.08380705,  0.3203804 , -0.0312923 ],
       [ 0.03694627,  0.31469744,  0.03162037],
       [ 0.10694021,  0.34435123,  0.01440988],
       [ 0.13280898,  0.34054166, -0.01345243],
       [ 0.11230974,  0.27350897,  0.00332716],
       [ 0.10086461,  0.30146822, -0.01458887],
       [ 0.05664819,  0.3867225 , -0.04182966],
       [-0.06259715,  0.25232798,  0.00277068],
       [ 0.13334315,  0.3458167 ,  0.01223674],
       [ 0.07516607,  0.31925476,  0.01165018],
       [ 0.09274146,  0.2431346 , -0.01880477],
       [ 0.07888874,  0.37314042,  0.05748507],
       [ 0.11637488,  0.23390415,  0.09996067],
       [ 0.20653449,  0.32336387,  0.09216474],
       [ 0.06796055,  0.29230502, -0.0219002 ],
       [ 0.13600177,  0.31489176,  0.03813237],
       [ 0.13665956,  0.43713355, -0.00129586],
       [ 0.12904686,  0.34506178,  0.008098  ],
       [ 0.12964919,  0.36074942,  0.07221845],
       [ 0.13998736,  0.4281543 ,  0.0411161 ],
       [-0.00210457,  0.29041094,  0.07396536],
       [ 0.18540733,  0.37413242,  0.00208285],
       [ 0.17939392,  0.3169523 ,  0.06321999],
       [ 0.07403658,  0.47988644,  0.11206155],
       [ 0.17828815,  0.34021235,  0.08241636],
       [ 0.05268158,  0.21780327, -0.00544531],
       [ 0.12965293,  0.34847856,  0.06868234],
       [ 0.11715467,  0.33093682,  0.10966303],
       [ 0.10650071,  0.41252735,  0.03430594],
       [ 0.07106532,  0.3329265 ,  0.01299649],
       [ 0.09665534,  0.3040476 , -0.00112819],
       [ 0.09029002,  0.40941164,  0.07586107],
       [ 0.11836594,  0.37746245,  0.05834516]], dtype=float32)), label_ids=None, metrics={'test_runtime': 0.3348, 'test_samples_per_second': 298.646, 'test_steps_per_second': 8.959})