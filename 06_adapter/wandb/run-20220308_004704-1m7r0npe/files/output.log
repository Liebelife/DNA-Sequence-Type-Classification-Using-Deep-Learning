loading configuration file /home/lieberze/DP/Thesis/05_model_training/roberta-trained-new-tokenizer_/config.json
Model config RobertaConfig {
  "adapters": {
    "adapters": {},
    "config_map": {},
    "fusion_config_map": {},
    "fusions": {}
  },
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "torch_dtype": "float32",
  "transformers_version": "4.11.3",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 5000
}
loading weights file /home/lieberze/DP/Thesis/05_model_training/roberta-trained-new-tokenizer_/pytorch_model.bin
Some weights of the model checkpoint at /home/lieberze/DP/Thesis/05_model_training/roberta-trained-new-tokenizer_ were not used when initializing RobertaModelWithHeads: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias']
- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at /home/lieberze/DP/Thesis/05_model_training/roberta-trained-new-tokenizer_ and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Adding adapter '3x3_fold_0'.
Adding head '3x3_fold_0' with config {'head_type': 'classification', 'num_labels': 3, 'layers': 2, 'activation_function': 'tanh', 'label2id': {'exon': 0, 'intergenic': 1, 'intron': 2}, 'use_pooler': False, 'bias': True}.
PyTorch: setting up devices
***** Running Prediction *****
  Num examples = 100
  Batch size = 48
/home/lieberze/.conda/envs/dp/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
total_optimization_steps: 19
how many 'eval_steps' to set: 2
how many steps in each evaluation stop: 2
hence in total: 4 steps for the whole evaluation
[array([[ 0.07428539,  0.28241438,  0.07199632],
       [ 0.09579688,  0.2572953 , -0.00951283],
       [ 0.06958276,  0.35519534,  0.09633713],
       [ 0.07756429,  0.34769845,  0.09981039],
       [ 0.09144208,  0.3462514 ,  0.11161989],
       [ 0.0806605 ,  0.17021027,  0.00614604],
       [ 0.05754358,  0.26490623,  0.15335552],
       [ 0.06916047,  0.27295107,  0.08131643],
       [ 0.07534575,  0.30100965,  0.09987634],
       [ 0.06212991,  0.23136649,  0.02677065],
       [ 0.06988095,  0.25500384,  0.07382555],
       [ 0.10593991,  0.28235728,  0.09175855],
       [ 0.08777731,  0.33113116,  0.10110446],
       [ 0.05406226,  0.3196715 ,  0.06452069],
       [ 0.0125846 ,  0.27323645,  0.12201681],
       [ 0.11770125,  0.18151748,  0.0369255 ],
       [ 0.13544494,  0.13885543,  0.00774219],
       [ 0.19903107,  0.15815571, -0.07807481],
       [ 0.08129157,  0.25162557,  0.10143841],
       [-0.0517035 ,  0.19060025,  0.06224805],
       [ 0.11272222,  0.30601513,  0.09562337],
       [ 0.11436791,  0.25833172,  0.05329591],
       [ 0.04916119,  0.3121882 ,  0.04493818],
       [ 0.0785564 ,  0.2916807 ,  0.03808304],
       [ 0.12727812,  0.26770955,  0.07015922],
       [ 0.07785208,  0.22488719,  0.0354127 ],
       [ 0.09397489,  0.3038065 ,  0.0097167 ],
       [ 0.08113661,  0.41620559,  0.1371946 ],
       [ 0.01959571,  0.26206726,  0.08570024],
       [ 0.09292957,  0.2405262 ,  0.06668836],
       [ 0.12355901,  0.4070667 ,  0.05009296],
       [ 0.12296804,  0.38494763,  0.1014011 ],
       [-0.01780351,  0.23155248,  0.13252154],
       [ 0.05526995,  0.19566488,  0.03603014],
       [-0.01442428,  0.11342107,  0.2244538 ],
       [ 0.14265876,  0.27762032,  0.06091104],
       [ 0.01007769,  0.25877473,  0.00182581],
       [ 0.06723984,  0.39647418,  0.06066401],
       [ 0.06726983,  0.3212952 ,  0.05222084],
       [ 0.05686639,  0.29497725,  0.02978199],
       [-0.08185934,  0.18562737,  0.12038345],
       [ 0.09107024,  0.24962643,  0.03611411],
       [ 0.10973905,  0.24077475,  0.05050647],
       [-0.05178817,  0.2328394 ,  0.09387294],
       [ 0.04899364,  0.28915408,  0.07034323],
       [ 0.15030058,  0.2902409 ,  0.06621814],
       [ 0.05694471,  0.42291015,  0.13180289],
       [ 0.12017176,  0.27046144,  0.17153479],
       [-0.11866589,  0.1128921 , -0.00200412],
       [ 0.09016818,  0.333456  ,  0.09428406],
       [ 0.067192  ,  0.28328046,  0.01930025],
       [ 0.07043134,  0.30482358,  0.07362491],
       [ 0.08233704,  0.2777691 ,  0.07373844],
       [ 0.07137484,  0.30741242,  0.07590933],
       [ 0.12265776,  0.33323288,  0.04688017],
       [ 0.07730171,  0.32197565,  0.10087593],
       [ 0.08489807,  0.2943645 ,  0.08394249],
       [ 0.07321436,  0.29690897,  0.10045014],
       [ 0.11123015,  0.3000586 ,  0.08830855],
       [ 0.04172322,  0.19403484,  0.22564471],
       [-0.03410812,  0.24703532,  0.03309238],
       [ 0.05700523,  0.32312077,  0.10949001],
       [ 0.05970949,  0.24885938,  0.04515191],
       [ 0.09330927,  0.35873824,  0.07730061],
       [ 0.09524506,  0.35820705,  0.10852468],
       [ 0.08342454,  0.34386927,  0.05679079],
       [ 0.04081191,  0.08504254,  0.00144328],
       [ 0.09325601,  0.23837173,  0.10354842],
       [ 0.08809657,  0.24201694,  0.03572604],
       [ 0.00560649,  0.2897038 ,  0.10225671],
       [ 0.07673804,  0.16160436,  0.07152797],
       [ 0.08208933,  0.2558285 ,  0.01445518],
       [ 0.12508349,  0.27061462,  0.06236761],
       [ 0.12197284,  0.29944754,  0.1057699 ],
       [ 0.06338315,  0.2783342 ,  0.10589562],
       [ 0.06930047,  0.19689783,  0.07307626],
       [ 0.12520933,  0.33013397,  0.0696226 ],
       [ 0.04819411,  0.22236976,  0.10807659],
       [ 0.07439277,  0.2635246 ,  0.05049788],
       [ 0.0442029 ,  0.27675405,  0.05353252],
       [ 0.12065942,  0.336016  ,  0.02652849],
       [ 0.10114706,  0.13064198,  0.05835447],
       [ 0.11954592,  0.20431171,  0.04281807],
       [ 0.19800055,  0.34410942,  0.14524743],
       [ 0.10039064,  0.2720977 ,  0.096443  ],
       [ 0.06588976,  0.33147907,  0.03457626],
       [ 0.05416105,  0.28265315,  0.06051512],
       [ 0.11998447,  0.21068422, -0.07914905],
       [ 0.09927447,  0.29619604,  0.06599063],
       [ 0.10432999,  0.19414237,  0.00367163],
       [ 0.08149833,  0.41310945,  0.12013817],
       [ 0.0688761 ,  0.2811783 ,  0.03282181],
       [ 0.03462622,  0.25399992,  0.19261302],
       [ 0.07817011,  0.34379393,  0.07063331],
       [ 0.04419611,  0.28208652,  0.02754404],
       [ 0.07042208,  0.31661123,  0.07229827],
       [ 0.084392  ,  0.32851562,  0.0878696 ],
       [ 0.08957172,  0.26588702,  0.03540668],
       [ 0.0584675 ,  0.2909878 ,  0.06836674],
       [ 0.04821111,  0.2901124 ,  0.09706177]], dtype=float32), 0         intron
1     intergenic
2         intron
3         intron
4         intron
         ...
95          exon
96        intron
97    intergenic
98          exon
99          exon
Name: type, Length: 100, dtype: object]
{'input_ids': tensor([[   0,   43,  344,  ...,    1,    1,    1],
        [   0,   56,  781,  ...,    1,    1,    1],
        [   0, 4673,  542,  ...,    1,    1,    1],
        ...,
        [   0, 3927,  390,  ...,    1,    1,    1],
        [   0,  360,  640,  ...,    1,    1,    1],
        [   0, 1543,  392,  ...,    1,    1,    1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([2, 1, 2, 2, 2, 0, 2, 2, 0, 1, 2, 2, 0, 0, 1, 2, 0, 2, 0, 1, 2, 0, 0, 0,
        2, 1, 0, 2, 2, 0, 0, 0, 1, 1, 2, 1, 2, 1, 0, 0, 2, 1, 0, 2, 0, 0, 0, 1])}